{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_index = 'SBBSSBSS'\n",
    "data_dir = '../../Data/Weather'\n",
    "measurements = ['TMIN', 'TMAX', 'TOBS', 'PRCP', 'SNOW', 'SNWD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./lib')\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pickle import load\n",
    "from numpy import linalg as LA\n",
    "from scipy.special import gammaln, factorial\n",
    "from sklearn.decomposition import PCA\n",
    "%pylab inline\n",
    "from ipyleaflet import (\n",
    "    Map,\n",
    "    Marker,\n",
    "    TileLayer, ImageOverlay,\n",
    "    Polyline, Polygon, Rectangle, Circle, CircleMarker,\n",
    "    GeoJSON,\n",
    "    DrawControl\n",
    ")\n",
    "\n",
    "from numpy_pack import packArray, unpackArray\n",
    "from spark_PCA import computeCov\n",
    "from computeStats import computeOverAllDist, STAT_Descriptions\n",
    "from YearPlotter import YearPlotter\n",
    "from Eigen_decomp import Eigen_decomp\n",
    "from recon_plot import recon_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "#sc.stop()\n",
    "sc = SparkContext(master=\"local[3]\",pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStats.py','lib/recon_plot.py','lib/Eigen_decomp.py'])\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pickle import dump\n",
    "\n",
    "# c_filename = 'US_Weather_%s.csv.gz'%file_index\n",
    "# u_filename = 'US_Weather_%s.csv'%file_index\n",
    "\n",
    "# url = \"https://mas-dse-open.s3.amazonaws.com/Weather/small\"\n",
    "# !curl $url/$c_filename > $data_dir/$c_filename \n",
    "# !gunzip -c $data_dir/$c_filename > $data_dir/$u_filename\n",
    "\n",
    "# List = pickle.load(open(data_dir+'/'+u_filename,'rb'))\n",
    "# df = sqlContext.createDataFrame(List)\n",
    "# print df.count()\n",
    "# df.write.save(data_dir+'/US_Weather_'+file_index+'.parquet')\n",
    "\n",
    "# sqlContext.registerDataFrameAsTable(df,'weather')\n",
    "# STAT = {}\n",
    "# for meas in measurements:\n",
    "#     Query = \"SELECT * FROM weather\\n\\tWHERE measurement = '%s'\"%meas\n",
    "#     print Query\n",
    "#     df = sqlContext.sql(Query)\n",
    "#     data = df.rdd.map(lambda row: unpackArray(row['vector'],np.float16))\n",
    "#     STAT[meas] = computeOverAllDist(data) \n",
    "#     OUT = computeCov(data)\n",
    "#     eigval,eigvec = LA.eig(OUT['Cov'])\n",
    "#     STAT[meas]['eigval'] = eigval\n",
    "#     STAT[meas]['eigvec'] = eigvec\n",
    "#     STAT[meas].update(OUT)\n",
    "\n",
    "# filename = data_dir+'/STAT_%s.pickle'%file_index\n",
    "# dump((STAT,STAT_Descriptions),open(data_dir+'/STAT_'+file_index+'.pickle','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "filename = data_dir+'/STAT_%s.pickle'%file_index\n",
    "STAT,STAT_Descriptions = load(open(filename,'rb'))\n",
    "filename = data_dir+'/US_Weather_%s.parquet'%file_index\n",
    "df = sqlContext.read.parquet(filename)\n",
    "print df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "months = [(0,31), (31,59), (59,90), (90,120), (120,151), (151,181), \\\n",
    "          (181,212), (212,243), (243,273), (273,304), (304,334), (334,365)]\n",
    "for i in months:\n",
    "    print '%.2f\\t%.2f\\t%.2f\\t%.2f'%(sum(STAT['TMIN']['Mean'][i[0]:i[1]])/(i[1]-i[0])/10, \\\n",
    "                                    sum(STAT['TOBS']['Mean'][i[0]:i[1]])/(i[1]-i[0])/10, \\\n",
    "                                    sum(STAT['TMAX']['Mean'][i[0]:i[1]])/(i[1]-i[0])/10, \\\n",
    "                                    sum(STAT['PRCP']['Mean'][i[0]:i[1]])/10)\n",
    "print sum(STAT['TMIN']['Mean'])/3650, sum(STAT['TMAX']['Mean'])/3650, sum(STAT['TOBS']['Mean'])/3650, \\\n",
    "      sum(STAT['PRCP']['Mean'])/10, sum(STAT['SNOW']['Mean'])/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot mean+-std\n",
    "fig,axes = plt.subplots(3, 2, figsize=(16,18))\n",
    "for i in range(6):\n",
    "    meas = measurements[i]\n",
    "    mean = STAT[meas]['Mean']\n",
    "    std = np.sqrt(STAT[meas]['Var'])\n",
    "    graph = np.vstack([(mean-std)/10,mean/10,(mean+std)/10]).transpose()\n",
    "    YearPlotter().plot(graph, fig, axes[i/2,i%2], title='mean+-std '+meas, labels=['mean-std','mean','mean+std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot eigenvalues\n",
    "fig,axes = plt.subplots(2, 3, sharex=True, figsize=(16,10))\n",
    "for i in range(6):\n",
    "    meas = measurements[i]\n",
    "    e_value = STAT[meas]['eigval']\n",
    "    print sum(e_value[:10])/sum(e_value)\n",
    "    subplot(2, 3, i+1)\n",
    "    plot(([0,]+list(cumsum(e_value[:10])))/sum(e_value))\n",
    "    title('percentage of variance explained for '+ meas)\n",
    "    ylabel('percentage of variance')\n",
    "    xlabel('# eigenvector')\n",
    "    grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot mean and top 5 eigenvectors\n",
    "meas = 'SNWD'\n",
    "fig,axes = plt.subplots(2, 1, figsize=(10,6))\n",
    "mean = STAT[meas]['Mean']\n",
    "e_vec = np.matrix(STAT[meas]['eigvec'][:,:3])\n",
    "YearPlotter().plot(mean/10, fig, axes[0], label='mean', title='mean '+meas)\n",
    "YearPlotter().plot(e_vec, fig, axes[1], labels=['eig1','eig2','eig3'], title='top 3 eigenvectors '+meas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot three station+year reconstruction by descending/ascending order\n",
    "meas = 'SNWD'\n",
    "mean = STAT[meas]['Mean']\n",
    "e_vec = [STAT[meas]['eigvec'][:,i] for i in range(3)]\n",
    "\n",
    "def decompose(row):\n",
    "    Series = np.array(unpackArray(row.vector,np.float16), dtype=np.float64)\n",
    "    recon = Eigen_decomp(None, Series/10, mean/10, e_vec);\n",
    "    total_var, residuals, reductions, coeff = recon.compute_var_explained()\n",
    "    residuals = [float(r) for r in residuals[1]]\n",
    "    coeff = [float(r) for r in coeff[1]]\n",
    "    D = row.asDict()\n",
    "    D['total_var'] = float(total_var[1])\n",
    "    D['res_mean'] = residuals[0]\n",
    "    for i in range(1,len(residuals)):\n",
    "        D['res_'+str(i)] = residuals[i]\n",
    "        D['coeff_'+str(i)] = coeff[i-1]\n",
    "    return Row(**D)\n",
    "\n",
    "rdd2 = df.rdd.filter(lambda row: row['measurement']==meas).map(decompose)\n",
    "df2 = sqlContext.createDataFrame(rdd2)\n",
    "df2 = df2.select('coeff_1','coeff_2','coeff_3','res_1','res_2','res_3','res_mean','vector') \\\n",
    "         .filter(df2.res_mean<0.99).filter(df2.res_1<0.99).filter(df2.res_2<0.99).filter(df2.res_3<0.99)\n",
    "\n",
    "def plot_recon_grid(which, ascending):\n",
    "    coeff = 'coeff_'+str(which)\n",
    "    res = 'res_'+str(which)\n",
    "    rows = df2.sort(coeff, ascending=ascending).take(4)\n",
    "    fig,axes = plt.subplots(1, 4, figsize=(16,4))\n",
    "    for i in range(4):\n",
    "        row = rows[i]\n",
    "        graph = [np.array(unpackArray(row.vector,np.float16), dtype=np.float64)/10, mean/10]\n",
    "        graph += [graph[-1] + row.coeff_1 * e_vec[0]]\n",
    "        graph += [graph[-1] + row.coeff_2 * e_vec[1]]\n",
    "        graph += [graph[-1] + row.coeff_3 * e_vec[2]]\n",
    "        graph = np.vstack(graph).transpose()\n",
    "        title = 'c%d=%.2f r%d=%.2f'%(which, row[coeff], which, row[res])\n",
    "        YearPlotter().plot(graph, fig, axes[i], labels=['target','mean','c1','c2','c3'], title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_recon_grid(1, False)\n",
    "plot_recon_grid(1, True)\n",
    "plot_recon_grid(2, False)\n",
    "plot_recon_grid(2, True)\n",
    "plot_recon_grid(3, False)\n",
    "plot_recon_grid(3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the CDF of coeff/res\n",
    "def plot_CDF(feature):\n",
    "    fig,axes = plt.subplots(1, 3, figsize=(16,5))\n",
    "    P = np.arange(0, 1, 1./df2.count())\n",
    "    if len(P) > df2.count():\n",
    "        P = P[:-1]\n",
    "    for i in range(3):\n",
    "        feat = feature+'_'+str(i+1)\n",
    "        rows = df2.select(feat).sort(feat).collect()\n",
    "        vals = [r[feat] for r in rows]\n",
    "        subplot(1, 3, i+1)\n",
    "        plot(vals,P)\n",
    "        title('cumulative distribution of '+feat)\n",
    "        ylabel('number of instances')\n",
    "        xlabel(feat)\n",
    "        grid()\n",
    "\n",
    "plot_CDF('coeff')\n",
    "plot_CDF('res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print minus MS of SNWD\n",
    "meas = 'SNWD'\n",
    "mean = STAT[meas]['Mean']\n",
    "e_vec = [STAT[meas]['eigvec'][:,i] for i in range(4)]\n",
    "\n",
    "rdd6 = df.rdd.filter(lambda row: row['measurement']==meas).map(decompose)\n",
    "df6 = sqlContext.createDataFrame(rdd6)\n",
    "df6 = df6.filter(df6.res_mean<0.999).filter(df6.res_1<0.999)\n",
    "    \n",
    "def MS(Mat):\n",
    "    return np.nanmean(Mat**2)\n",
    "\n",
    "for i in range(4):\n",
    "    coef = 'coeff_%d'%(i+1)\n",
    "    year_station_table = df6.select('station', 'year', coef).toPandas() \\\n",
    "                            .pivot(index='year', columns='station', values=coef)\n",
    "    mean_by_year = np.nanmean(year_station_table,axis=1)\n",
    "    mean_by_station = np.nanmean(year_station_table,axis=0)\n",
    "    tbl_minus_year = (year_station_table.transpose()-mean_by_year).transpose()\n",
    "    tbl_minus_station = year_station_table-mean_by_station\n",
    "    print 'total MS                    = ', MS(year_station_table)\n",
    "    var_e = 1 - MS(tbl_minus_station) / MS(year_station_table)\n",
    "    print 'MS removing mean-by-station = ', MS(tbl_minus_station), 'variance explained = ', var_e\n",
    "    var_e = 1 - MS(tbl_minus_year) / MS(year_station_table)\n",
    "    print 'MS removing mean-by-year    = ', MS(tbl_minus_year), 'variance explained = ', var_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot P_norm\n",
    "def fillnans(l):\n",
    "    x = np.zeros(l)\n",
    "    x.fill(np.nan)\n",
    "    return x\n",
    "\n",
    "meas = 'PRCP'\n",
    "rows = df.rdd.filter(lambda row: row['measurement']==meas) \\\n",
    "             .map(lambda row:(row.station,row.year,unpackArray(row['vector'],np.float16))).collect()\n",
    "years = set([r[1] for r in rows])\n",
    "days = int((max(years)-min(years)+1)*365)\n",
    "stations = sorted(list(set([r[0] for r in rows])))\n",
    "A_dict = {st: fillnans(days) for st in stations}\n",
    "for i in range(len(rows)):\n",
    "    row = rows[i]\n",
    "    loc = int((row[1]-min(years))*365)\n",
    "    A_dict[row[0]][loc:loc+365] = row[2]\n",
    "A_list = [A_dict[st] for st in stations]\n",
    "A = np.hstack([A_list])\n",
    "\n",
    "def G(n):\n",
    "    return gammaln(n+1)\n",
    "\n",
    "def LogProb(m,l,n1,n2):\n",
    "    logP = -G(l)-G(n1-l)-G(n2-l)-G(m-n1-n2+l)-G(m)+G(n1)+G(m-n1)+G(n2)+G(m-n2)\n",
    "    return logP / m\n",
    "\n",
    "def computeLogProb(X, Y):\n",
    "    X[np.isnan(Y)] = np.nan\n",
    "    Y[np.isnan(X)] = np.nan\n",
    "    G = ~isnan(X)\n",
    "    m = sum(G)\n",
    "    XG = X[G]>0\n",
    "    YG = Y[G]>0\n",
    "    n1 = sum(XG)\n",
    "    n2 = sum(YG)\n",
    "    l = sum(XG*YG)\n",
    "    logprob = LogProb(m,l,n1,n2)\n",
    "    return logprob, m\n",
    "\n",
    "l_stations = len(stations)\n",
    "Length = np.zeros([l_stations,l_stations])\n",
    "P_norm = np.zeros([l_stations,l_stations])\n",
    "for i in range(l_stations):\n",
    "    for j in range(l_stations):\n",
    "        if i==j: \n",
    "            P_norm[i,j] = -0.4\n",
    "            continue\n",
    "        X = copy(A_dict[stations[i]])\n",
    "        Y = copy(A_dict[stations[j]])\n",
    "        P_norm[i,j], Length[i,j] = computeLogProb(X,Y)\n",
    "        if Length[i,j] < 200:\n",
    "            P_norm[i,j] = np.nan\n",
    "\n",
    "P_norm_flat = P_norm.flatten();\n",
    "P_norm_flat_nn = P_norm_flat[~isnan(P_norm_flat)]\n",
    "hist(-P_norm_flat_nn, bins=100);\n",
    "xlabel('significance')            \n",
    "\n",
    "P_norm0 = np.nan_to_num(P_norm)\n",
    "pca = PCA(n_components=3, svd_solver='full')\n",
    "pca.fit(P_norm0)\n",
    "fig,axes = plt.subplots(2, 2, figsize=(16,16))\n",
    "axes[0,0].imshow(P_norm, cmap=plt.cm.gray)\n",
    "axes[0,0].set_title('P_norm original')\n",
    "for i in range(3):\n",
    "    order = np.argsort(pca.components_[i,:])\n",
    "    P_norm_reord = P_norm0[order,:]\n",
    "    P_norm_reord = P_norm_reord[:,order]\n",
    "    axes[i>0,(i+1)%2].matshow(P_norm_reord)\n",
    "    axes[i>0,(i+1)%2].set_title('P_norm reodered by eig%d'%(i+1))\n",
    "\n",
    "fig,axes = plt.subplots(1, 3, figsize=(16,5))\n",
    "P_norm_eig = list(pca.components_.transpose())\n",
    "for i in range(3):\n",
    "    P_norm_eig_order = sorted(P_norm_eig, key=lambda x:x[i])\n",
    "    subplot(1, 3, i+1)\n",
    "    plot(P_norm_eig_order)\n",
    "    title('top 3 eigenvectors ordered by eig%d'%(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot map of temperature\n",
    "meas = 'TOBS'\n",
    "mean = STAT[meas]['Mean']\n",
    "e_vec = [STAT[meas]['eigvec'][:,i] for i in range(4)]\n",
    "rdd3 = df.rdd.filter(lambda row: row['measurement']==meas).map(decompose)\n",
    "df3 = sqlContext.createDataFrame(rdd3)\n",
    "df3 = df3.filter(df3.res_1<0.99).filter(df3.res_2<0.99).filter(df3.res_3<0.99).filter(df3.res_4<0.99) \\\n",
    "         .select('station','longitude','latitude','coeff_1','coeff_2','coeff_3','coeff_4') \\\n",
    "         .groupby(['station','longitude','latitude']) \\\n",
    "         .agg({'coeff_1':'mean', 'coeff_2':'mean', 'coeff_3':'mean', 'coeff_4':'mean'})\n",
    "\n",
    "st_dict = df3.rdd.map(lambda row: ((row[0],row[1],row[2]),(row[3],row[4],row[5],row[6]))).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "\n",
    "coeffs = [st_dict[st] for st in st_dict]\n",
    "coeffs_absmax = np.max(np.abs(coeffs), axis=0)\n",
    "\n",
    "longitudes = [st[1] for st in st_dict]\n",
    "min_long, max_long = min(longitudes)-0.1, max(longitudes)+0.1\n",
    "latitudes = [st[2] for st in st_dict]\n",
    "min_lat, max_lat = min(latitudes)-0.1, max(latitudes)+0.1\n",
    "center = [(min_lat+max_lat)/2, (min_long+max_long)/2]\n",
    "myMap = Map(default_tiles=TileLayer(opacity=0.6), center=center, zoom=7)\n",
    "myRect = Rectangle(bounds=[[min_lat,min_long],[max_lat,max_long]], weight=5, fill_opacity=0.0)\n",
    "myMap += myRect\n",
    "\n",
    "for st in st_dict:\n",
    "    _long = st[1]\n",
    "    _lat = st[2]\n",
    "    for i in range(4):\n",
    "        _coeff = st_dict[st][i]\n",
    "        if np.isnan(_coeff):\n",
    "            continue\n",
    "        r = _coeff / coeffs_absmax[i] / 5\n",
    "        color = colors[i]\n",
    "        signs = [[-1,+1],[+1,+1],[-1,-1],[+1,-1]]\n",
    "        long_sign, lat_sign = signs[i]\n",
    "        if _coeff >= 0:\n",
    "            triangle = [(_lat,_long),(_lat+lat_sign*r,_long),(_lat,_long+long_sign*r),(_lat,_long)]\n",
    "            myPoly = Polygon(locations=triangle, weight=0, color=color, opacity=0, fill_opacity=0.7, fill_color=color)\n",
    "        else:\n",
    "            triangle = [(_lat,_long),(_lat-lat_sign*r,_long),(_lat,_long-long_sign*r),(_lat,_long)]\n",
    "            myPoly = Polygon(locations=triangle, weight=2, color=color, opacity=0.8, fill_opacity=0, fill_color=color)\n",
    "        myMap += myPoly\n",
    "myMap   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# monthly PRCP\n",
    "m = 'PRCP'\n",
    "def sum_over_month(row):\n",
    "    months = [(0,31), (31,59), (59,90), (90,120), (120,151), (151,181), \\\n",
    "              (181,212), (212,243), (243,273), (273,304), (304,334), (334,365)]\n",
    "    vec = np.array(unpackArray(row.vector, np.float16), dtype=np.float64)\n",
    "    vec_month = np.array([sum(vec[i[0]:i[1]]) for i in months])\n",
    "    return vec_month\n",
    "\n",
    "rdd4 = df.rdd.filter(lambda row: row['measurement']==m).map(lambda row: sum_over_month(row))\n",
    "prcp_m = np.vstack(rdd4.collect())\n",
    "\n",
    "mean = np.nanmean(prcp_m, axis=0)\n",
    "std = np.nanstd(prcp_m, axis=0)\n",
    "pca_prcp = PCA(n_components=12, svd_solver='full')\n",
    "pca_prcp.fit(np.nan_to_num(prcp_m))\n",
    "eigvec_prcp = pca_prcp.components_\n",
    "eigval_prcp = pca_prcp.explained_variance_\n",
    "\n",
    "P = range(1,13)\n",
    "Months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Des']\n",
    "\n",
    "plt.plot(P, (mean-std)/10, label='mean-std')\n",
    "plt.plot(P, mean/10, label='mean')\n",
    "plt.plot(P, (mean+std)/10, label='mean+std')\n",
    "title('mean+-std of monthly '+ m)\n",
    "xticks(P, Months)\n",
    "legend()\n",
    "grid()\n",
    "\n",
    "fig,axes = plt.subplots(1, 2, figsize=(16,7))\n",
    "\n",
    "subplot(1,2,1)\n",
    "plot([0] + P, ([0] + list(cumsum(eigval_prcp))) / sum(eigval_prcp))\n",
    "title('% of variance explained for monthly '+ m)\n",
    "ylabel('percentage of variance')\n",
    "xlabel('# eigenvector')\n",
    "grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for i in range(8):\n",
    "    plot(P, eigvec_prcp[i], label='eig%d'%(i+1))\n",
    "title('top 8 eigenvectors of monthly ' + m)\n",
    "xticks(P, Months)\n",
    "legend()\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decompose2(Series):\n",
    "    recon = Eigen_decomp(None, Series/10, mean/10, eigvec_prcp);\n",
    "    total_var, residuals, reductions, coeff = recon.compute_var_explained()\n",
    "    residuals = [float(r) for r in residuals[1]]\n",
    "    coeff = [float(r) for r in coeff[1]]\n",
    "    D = {}\n",
    "    for i in range(1,len(residuals)):\n",
    "        D['res_'+str(i)] = residuals[i]\n",
    "        D['coeff_'+str(i)] = coeff[i-1]\n",
    "    return Row(**D)\n",
    "\n",
    "rdd5 = rdd4.map(decompose2)\n",
    "df5 = sqlContext.createDataFrame(rdd5)\n",
    "df5 = df5.filter(df5.res_1<0.99).filter(df5.res_2<0.99).filter(df5.res_3<0.99).filter(df5.res_4<0.99) \\\n",
    "         .filter(df5.res_5<0.99).filter(df5.res_6<0.99).filter(df5.res_7<0.99).filter(df5.res_8<0.99)\n",
    "\n",
    "def plot_CDF2(feature):\n",
    "#     fig,axes = plt.subplots(2, 4, figsize=(16,9))\n",
    "    P = np.arange(0, 1, 1./df5.count())\n",
    "    if len(P) > df5.count():\n",
    "        P = P[:-1]\n",
    "    for i in range(8):\n",
    "        feat = feature+'_'+str(i+1)\n",
    "        rows = df5.select(feat).sort(feat).collect()\n",
    "        vals = [r[feat] for r in rows]\n",
    "#         subplot(2,4,i+1)\n",
    "        plot(vals, P, label='res_%d'%(i+1))\n",
    "    title('cumulative distribution of residual variance')\n",
    "    ylabel('number of instances')\n",
    "#     xlabel(feat)\n",
    "    grid()\n",
    "    legend()\n",
    "\n",
    "# plot_CDF2('coeff')\n",
    "plot_CDF2('res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
