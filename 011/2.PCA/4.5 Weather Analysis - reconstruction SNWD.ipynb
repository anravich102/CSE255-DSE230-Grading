{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#setup\n",
    "data_dir='../../Data/Weather'\n",
    "file_index='BBSBSBSB'\n",
    "m='SNWD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "variables": {
     "m": "SNWD"
    }
   },
   "source": [
    "## Reconstruction using top eigen-vectors\n",
    "For measurement = {{m}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Enable automiatic reload of libraries\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2 # means that all modules are reloaded before every command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import sys\n",
    "sys.path.append('./lib')\n",
    "\n",
    "from numpy_pack import packArray,unpackArray\n",
    "\n",
    "from Eigen_decomp import Eigen_decomp\n",
    "from YearPlotter import YearPlotter\n",
    "from recon_plot import recon_plot\n",
    "\n",
    "from import_modules import import_modules,modules\n",
    "import_modules(modules)\n",
    "\n",
    "from ipywidgets import interactive,widgets\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "#sc.stop()\n",
    "\n",
    "sc = SparkContext(master=\"local[3]\",pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStats.py','lib/recon_plot.py','lib/Eigen_decomp.py'])\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read Statistics File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "#read statistics\n",
    "filename=data_dir+'/STAT_%s.pickle'%file_index\n",
    "STAT,STAT_Descriptions = load(open(filename,'rb'))\n",
    "measurements=STAT.keys()\n",
    "print 'keys from STAT=',measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read data file into a spark DataFrame\n",
    "We focus on the snow-depth records, because the eigen-vectors for them make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "filename=data_dir+'/US_Weather_%s.parquet'%file_index\n",
    "df_in=sqlContext.read.parquet(filename)\n",
    "#filter in \n",
    "df=df_in.filter(df_in.measurement==m)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plot Reconstructions\n",
    "\n",
    "Construct approximations of a time series using the mean and the $k$ top eigen-vectors\n",
    "First, we plot the mean and the top $k$ eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "fig,axes=plt.subplots(2,1, sharex='col', sharey='row',figsize=(10,6));\n",
    "k=3\n",
    "EigVec=np.matrix(STAT[m]['eigvec'][:,:k])\n",
    "Mean=STAT[m]['Mean']\n",
    "YearPlotter().plot(Mean,fig,axes[0],label='Mean',title=m+' Mean')\n",
    "YearPlotter().plot(EigVec,fig,axes[1],title=m+' Eigs',labels=['eig'+str(i+1) for i in range(k)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "v=[np.array(EigVec[:,i]).flatten() for i in range(np.shape(EigVec)[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### plot the percent of residual variance on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  x=0 in the graphs below correspond to the fraction of the variance explained by the mean alone\n",
    "#  x=1,2,3,... are the residuals for eig1, eig1+eig2, eig1+eig2+eig3 ...\n",
    "fig,ax=plt.subplots(1,1);\n",
    "eigvals=STAT[m]['eigval']; eigvals/=sum(eigvals); cumvar=np.cumsum(eigvals); cumvar=100*np.insert(cumvar,0,0)\n",
    "ax.plot(cumvar[:4], 'o-'); \n",
    "ax.grid(); \n",
    "ax.set_ylabel('Percent of variance explained')\n",
    "ax.set_xlabel('number of eigenvectors')\n",
    "ax.set_title('Percent of variance explained');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Process whole dataframe to find best and worse residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Add to each row in the dataframe a residual values \n",
    "Residuals are after subtracting in sequence: the mean, the projection on the first eigen-vector the projection on the second eigen-vector etc.\n",
    "\n",
    "`decompose(row)` axtracts the series from the row, computes the residuals and constructs a new row that is reassembled into a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def decompose(row):\n",
    "    \"\"\"compute residual and coefficients for decomposition           \n",
    "\n",
    "    :param row: SparkSQL Row that contains the measurements for a particular station, year and measurement. \n",
    "    :returns: the input row with additional information from the eigen-decomposition.\n",
    "    :rtype: SparkSQL Row \n",
    "\n",
    "    Note that Decompose is designed to run inside a spark \"map()\" command.\n",
    "    Mean and v are sent to the workers as local variables of \"Decompose\"\n",
    "\n",
    "    \"\"\"\n",
    "    Series=np.array(unpackArray(row.vector,np.float16),dtype=np.float64)\n",
    "    recon=Eigen_decomp(None,Series,Mean,v);\n",
    "    total_var,residuals,reductions,coeff=recon.compute_var_explained()\n",
    "    #print coeff\n",
    "    residuals=[float(r) for r in residuals[1]]\n",
    "    coeff=[float(r) for r in coeff[1]]\n",
    "    D=row.asDict()\n",
    "    D['total_var']=float(total_var[1])\n",
    "    D['res_mean']=residuals[0]\n",
    "    for i in range(1,len(residuals)):\n",
    "        D['res_'+str(i)]=residuals[i]\n",
    "        D['coeff_'+str(i)]=coeff[i-1]\n",
    "    return Row(**D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "rdd2=df.rdd.map(decompose)\n",
    "df2=sqlContext.createDataFrame(rdd2)\n",
    "row,=df2.take(1)\n",
    "\n",
    "#filter out vectors for which the mean is a worse approximation than zero.\n",
    "print 'before filter',df2.count()\n",
    "df3=df2.filter(df2.res_mean<1)\n",
    "print 'after filter',df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "nanmean sometimes gives infinity when\n",
    "there are nan and the other values are float16.\n",
    "so convert first to float64 seems to solve this.\n",
    "http://stackoverflow.com/questions/24313649/why-does-numpy-mean-return-inf\n",
    "'''\n",
    "def customMean(x):\n",
    "    return np.nanmean(np.float64(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4));\n",
    "YP=YearPlotter()\n",
    "\n",
    "interval = 10 # years between \n",
    "startYear = 1949\n",
    "\n",
    "dfSnow=df2.filter(df2.measurement==m).filter(df2.year < (startYear+interval)) \n",
    "\n",
    "rows=dfSnow.rdd.map(lambda row:unpackArray(row['vector'],np.float16)).collect()\n",
    "\n",
    "Tbaseline=customMean(np.vstack(rows))\n",
    "YP.plot(Tbaseline.transpose(),fig,ax,title=m)\n",
    "\n",
    "l = ['{}-{}'.format(startYear, startYear+interval)]\n",
    "\n",
    "for i in range(startYear+interval, 2000, interval):\n",
    "\n",
    "    dfSnow=df2.filter(df2.measurement==m).filter(df2.year >= i).filter(df2.year < (i+interval))\n",
    "\n",
    "    rows=dfSnow.rdd.map(lambda row:unpackArray(row['vector'],np.float16)).collect()\n",
    "\n",
    "    T=customMean(np.vstack(rows))  #- Tbaseline\n",
    "    YP.plot(T.transpose(),fig,ax,title=m);\n",
    "    l += ['{}-{}'.format(i, i+interval)]\n",
    "    \n",
    "plt.legend(l);\n",
    "plt.grid();\n",
    "ylabel('Snow Depth (mm)');\n",
    "plt.savefig('SNWD.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10));\n",
    "YP=YearPlotter()\n",
    "\n",
    "#interval = 20 # years between \n",
    "#startYear = 1949\n",
    "\n",
    "dfSnow=df2.filter(df2.measurement==m).filter(df2.year < (startYear+interval)) \n",
    "\n",
    "rows=dfSnow.rdd.map(lambda row:unpackArray(row['vector'],np.float16)).collect()\n",
    "\n",
    "Tbaseline=customMean(np.vstack(rows))\n",
    "#YP.plot(Tbaseline.transpose(),fig,ax,title=m)\n",
    "\n",
    "l = [] #['{}-{}'.format(startYear, startYear+interval)]\n",
    "\n",
    "for i in range(startYear+interval, 2000, interval):\n",
    "\n",
    "    dfSnow=df2.filter(df2.measurement==m).filter(df2.year >= i).filter(df2.year < (i+interval))\n",
    "\n",
    "    rows=dfSnow.rdd.map(lambda row:unpackArray(row['vector'],np.float16)).collect()\n",
    "\n",
    "    T=customMean(np.vstack(rows))  - Tbaseline\n",
    "    YP.plot(T.transpose(),fig,ax,title=m);\n",
    "    l += ['{}-{}'.format(i, i+interval)]\n",
    "    \n",
    "plt.legend(l);\n",
    "plt.grid();\n",
    "plt.title('Change in SNWD compared to {}'.format('{}-{}'.format(startYear, startYear+interval)));\n",
    "ylabel('Change in SNWD (mm)');\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's scale each SNWD graph by the maximum snow for the year. This way we can compare the build up and melt down of the snow between years (rather than the absolute snow change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10));\n",
    "YP=YearPlotter()\n",
    "\n",
    "#interval = 20 # years between \n",
    "#startYear = 1949\n",
    "\n",
    "dfSnow=df2.filter(df2.measurement==m).filter(df2.year < (startYear+interval)) \n",
    "\n",
    "rows=dfSnow.rdd.map(lambda row:unpackArray(row['vector'],np.float16)).collect()\n",
    "\n",
    "Tbaseline=customMean(np.vstack(rows))\n",
    "Tbaseline /= Tbaseline.sum()\n",
    "YP.plot(Tbaseline.transpose(),fig,ax,title=m)\n",
    "\n",
    "l = ['{}-{}'.format(startYear, startYear+interval)]\n",
    "\n",
    "for i in range(startYear+interval, 2000, interval):\n",
    "\n",
    "    dfSnow=df2.filter(df2.measurement==m).filter(df2.year >= i).filter(df2.year < (i+interval))\n",
    "\n",
    "    rows=dfSnow.rdd.map(lambda row:unpackArray(row['vector'],np.float16)).collect()\n",
    "\n",
    "    T=customMean(np.vstack(rows))  \n",
    "    T /= T.sum()\n",
    "    #T -= Tbaseline\n",
    "    YP.plot(T.transpose(),fig,ax,title=m);\n",
    "    l += ['{}-{}'.format(i, i+interval)]\n",
    "    \n",
    "plt.legend(l);\n",
    "plt.grid();\n",
    "plt.title('Normalized SNWD compared to {}'.format('{}-{}'.format(startYear, startYear+interval)));\n",
    "ylabel('Normalized SNWD (% of total snow for year)');\n",
    "plt.grid();\n",
    "plt.savefig('NormalizedSnow.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8));\n",
    "YP=YearPlotter()\n",
    "\n",
    "interval = 10 # years between \n",
    "#startYear = 1949\n",
    "\n",
    "dfSnow=df2.filter(df2.measurement==m).filter(df2.year < (startYear+interval)) \n",
    "\n",
    "rows=dfSnow.rdd.map(lambda row:unpackArray(row['vector'],np.float16)).collect()\n",
    "\n",
    "Tbaseline=customMean(np.vstack(rows))\n",
    "Tbaseline /= Tbaseline.sum()\n",
    "#YP.plot(Tbaseline.transpose(),fig,ax,title=m)\n",
    "\n",
    "l = [] #['{}-{}'.format(startYear, startYear+interval)]\n",
    "\n",
    "for i in range(startYear+interval, 2000, interval):\n",
    "\n",
    "    dfSnow=df2.filter(df2.measurement==m).filter(df2.year >= i).filter(df2.year < (i+interval))\n",
    "\n",
    "    rows=dfSnow.rdd.map(lambda row:unpackArray(row['vector'],np.float16)).collect()\n",
    "\n",
    "    T=customMean(np.vstack(rows))  \n",
    "    T /= T.sum()\n",
    "    T -= Tbaseline\n",
    "    YP.plot(T.transpose(),fig,ax,title=m);\n",
    "    l += ['{}-{}'.format(i, i+interval)]\n",
    "    \n",
    "plt.legend(l);\n",
    "plt.grid();\n",
    "plt.title('Change in normalized SNWD compared to {}'.format('{}-{}'.format(startYear, startYear+interval)));\n",
    "ylabel('Change in normalized SNWD');\n",
    "plt.grid();\n",
    "plt.savefig('NormalizedSnowChange.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how the \"No Snow\" part of the year changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findLongestRun(x):\n",
    "    '''\n",
    "    Searches a vector for the longest continuous sequence (e.g. 13,14,15,16).\n",
    "    If there are multiple equal runs, then the first run is returned.\n",
    "\n",
    "    e.g. \n",
    "    findLongestRun([4, 5, 6, 10,11,12,13, 123, 124, 125, 126])\n",
    "    returns [3, 4] because 10-13 is the longest sequence\n",
    "    \n",
    "    findLongestRun([4, 5, 6, 7, 8, 10,11,12,13, 123, 124, 125, 126])\n",
    "    returns [0, 5] because 4-8 is the longest sequence\n",
    "    \n",
    "    findLongestRun([-3,-2, -4, 123, 124, 125, 126, 127, 128, 10,11,12,13, 9123, 9124, 8125, 8126])\n",
    "    returns [3,6] because 123-128 is the longest sequence\n",
    "    '''\n",
    "    \n",
    "    indexofRuns = np.where(np.insert(np.diff(x),0,1) != 1)[0]\n",
    "    indexofRuns = np.insert(np.insert(indexofRuns, len(indexofRuns), len(x)), 0, 0)\n",
    "    \n",
    "    runs = np.diff(indexofRuns)\n",
    "    \n",
    "    longestRun = np.argmax(runs)\n",
    "    \n",
    "    longestRunLength = np.max(runs)\n",
    "    \n",
    "    return [indexofRuns[longestRun], longestRunLength]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfSnow=df2.filter(df2.measurement==m).filter(df2.year == 1949)\n",
    "\n",
    "rows=dfSnow.rdd.map(lambda row:unpackArray(row['vector'],np.float16)).collect()\n",
    "\n",
    "yearsArray = [1949]\n",
    "T = rows[0]\n",
    "    \n",
    "noSnowDays = np.where((T < 2) | (np.isnan(T)))[0]  # \"Snow melted if < 2 mm depth\"\n",
    "[noSnowStart, noSnowLength] = findLongestRun(noSnowDays)\n",
    "\n",
    "## Sanity check\n",
    "# I've found 8 occurences between 1998 and 2010 where SNWD is reported 0 or nan for all 365 days\n",
    "# This is most likely a problem with the station or the data. I doubt that the station\n",
    "# actually had 365 days without a single mm of snowfall in northern Minnesota.\n",
    "# So let's say that if there are more than 340 days without snow in a year, then \n",
    "# we don't count that in the analysis as it is probably an error.\n",
    "if (noSnowLength < 340):\n",
    "    noSnowPeriod = np.array([noSnowDays[noSnowStart], noSnowDays[(noSnowStart + noSnowLength - 1)]])\n",
    "\n",
    "for i in range(1,np.shape(rows)[0]):\n",
    "    \n",
    "    T = rows[i]\n",
    "    \n",
    "    noSnowDays = np.where((T < 2) | (np.isnan(T)))[0]  # \"Snow melted if < 2 mm depth\"\n",
    "    [noSnowStart, noSnowLength] = findLongestRun(noSnowDays)\n",
    "    if (noSnowLength < 340):\n",
    "        noSnowPeriod = np.array([noSnowDays[noSnowStart], noSnowDays[(noSnowStart + noSnowLength - 1)]])\n",
    "        yearsArray.append(1949)\n",
    "    \n",
    "for i in range(1950,2012):\n",
    "    \n",
    "    dfSnow=df2.filter(df2.measurement==m).filter(df2.year == i)\n",
    "\n",
    "    rows=dfSnow.rdd.map(lambda row:unpackArray(row['vector'],np.float16)).collect()\n",
    "\n",
    "    for j in range(np.shape(rows)[0]):\n",
    "    \n",
    "        T = rows[j]\n",
    "\n",
    "        noSnowDays = np.where((T < 2) | (np.isnan(T)))[0] # \"Snow melted if < 2 mm depth\"\n",
    "        [noSnowStart, noSnowLength] = findLongestRun(noSnowDays)\n",
    "\n",
    "        if (noSnowLength < 340):\n",
    "            noSnowPeriod = np.vstack([noSnowPeriod, np.array([noSnowDays[noSnowStart], \\\n",
    "                                                             noSnowDays[(noSnowStart + noSnowLength - 1)]])])\n",
    "            yearsArray.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1949 = noSnowPeriod[np.where(np.array(yearsArray)==1949)[0]][:,1] - \\\n",
    "noSnowPeriod[np.where(np.array(yearsArray)==1949)[0]][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1955 = noSnowPeriod[np.where(np.array(yearsArray)==1955)[0]][:,1] - \\\n",
    "noSnowPeriod[np.where(np.array(yearsArray)==1955)[0]][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1989 = noSnowPeriod[np.where(np.array(yearsArray)==1989)[0]][:,1] - \\\n",
    "noSnowPeriod[np.where(np.array(yearsArray)==1989)[0]][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1999 = noSnowPeriod[np.where(np.array(yearsArray)==1999)[0]][:,1] - \\\n",
    "noSnowPeriod[np.where(np.array(yearsArray)==1999)[0]][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a2009 = noSnowPeriod[np.where(np.array(yearsArray)==2009)[0]][:,1] - \\\n",
    "noSnowPeriod[np.where(np.array(yearsArray)==2009)[0]][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1979 = noSnowPeriod[np.where(np.array(yearsArray)==1979)[0]][:,1] - \\\n",
    "noSnowPeriod[np.where(np.array(yearsArray)==1979)[0]][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1969 = noSnowPeriod[np.where(np.array(yearsArray)==1969)[0]][:,1] - \\\n",
    "noSnowPeriod[np.where(np.array(yearsArray)==1969)[0]][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1959 = noSnowPeriod[np.where(np.array(yearsArray)==1959)[0]][:,1] - \\\n",
    "noSnowPeriod[np.where(np.array(yearsArray)==1959)[0]][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(a2009, alpha=0.75, label='2009', normed=True);\n",
    "plt.hist(a1949, alpha=0.75, color='r', label='1949', normed=True);\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(a2009, alpha=0.75, label='2009');\n",
    "for i in range(1949,1955):\n",
    "    \n",
    "    s1 = noSnowPeriod[np.where(np.array(yearsArray)==i)[0]][:,1] - \\\n",
    "         noSnowPeriod[np.where(np.array(yearsArray)==i)[0]][:,0]\n",
    "    plt.hist(s1, alpha=0.75, label=str(i));\n",
    "    \n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "season = np.zeros([len(range(1949,2011)),4])\n",
    "j = 0\n",
    "for i in range(1949,2011):\n",
    "    lengths = noSnowPeriod[np.where(np.array(yearsArray)==i)[0]][:,1] - \\\n",
    "              noSnowPeriod[np.where(np.array(yearsArray)==i)[0]][:,0]\n",
    "    season[j,0] = np.mean(lengths)\n",
    "    season[j,1] = np.var(lengths) #np.std(lengths)\n",
    "    season[j,2] = np.mean(noSnowPeriod[np.where(np.array(yearsArray)==i)[0]][:,0])\n",
    "    season[j,3] = np.var(noSnowPeriod[np.where(np.array(yearsArray)==i)[0]][:,0]) #np.std(noSnowPeriod[np.where(np.array(yearsArray)==i)[0]][:,0])\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = linregress(range(len(season[:,0])),season[:,0])\n",
    "print ('Mean Length of \"no snow\" season, p_value = {:.5f}, r2 = {:.3f}'.format(p_value, r_value**2))\n",
    "if (p_value < 0.05):\n",
    "    print('Statistically significant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = linregress(range(len(season[:,1])),season[:,1])\n",
    "print ('Mean Length Variance of \"no snow\" season, p_value = {:.5f}, r2 = {:.3f}'.format(p_value, r_value**2))\n",
    "if (p_value < 0.05):\n",
    "    print('Statistically significant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = linregress(range(len(season[:,2])),season[:,2])\n",
    "print ('Mean Start Day of \"no snow\" season, p_value = {:.5f}, r2 = {:.3f}'.format(p_value, r_value**2))\n",
    "if (p_value < 0.05):\n",
    "    print('Statistically significant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = linregress(range(len(season[:,3])),season[:,3])\n",
    "print ('Mean Start Day Variance of \"no snow\" season, p_value = {:.5f}, r2 = {:.3f}'.format(p_value, r_value**2))\n",
    "if (p_value < 0.05):\n",
    "    print('Statistically significant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(range(1949, 2011), season[:,0], yerr=season[:,1]);\n",
    "plt.xlabel('Year');\n",
    "plt.ylabel('\"No snow\" season mean and variance');\n",
    "plt.title('\"No snow\" Season Variance Over Time');\n",
    "plt.savefig('SNWDVarianceOverTime.svg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(range(1949, 2011), season[:,2], yerr=season[:,3]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1949, 2011), season[:,1], 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(season[:,2]);  # First day of \"no snow\" season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1949, 2011), season[:,3], 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Sort entries by increasing values of ers_3\n",
    "df3=df3.sort(df3.res_3,ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_decomp(row,Mean,v,fig=None,ax=None,Title=None,interactive=False):\n",
    "    \"\"\"Plot a single reconstruction with an informative title\n",
    "\n",
    "    :param row: SparkSQL Row that contains the measurements for a particular station, year and measurement. \n",
    "    :param Mean: The mean vector of all measurements of a given type\n",
    "    :param v: eigen-vectors for the distribution of measurements.\n",
    "    :param fig: a matplotlib figure in which to place the plot\n",
    "    :param ax: a matplotlib axis in which to place the plot\n",
    "    :param Title: A plot title over-ride.\n",
    "    :param interactive: A flag that indicates whether or not this is an interactive plot (widget-driven)\n",
    "    :returns: a plotter returned by recon_plot initialization\n",
    "    :rtype: recon_plot\n",
    "\n",
    "    \"\"\"\n",
    "    target=np.array(unpackArray(row.vector,np.float16),dtype=np.float64)\n",
    "    if Title is None:\n",
    "        Title='%s / %d    %s'%(row['station'],row['year'],row['measurement'])\n",
    "    eigen_decomp=Eigen_decomp(range(1,366),target,Mean,v)\n",
    "    plotter=recon_plot(eigen_decomp,year_axis=True,fig=fig,ax=ax,interactive=interactive,Title=Title)\n",
    "    return plotter\n",
    "\n",
    "def plot_recon_grid(rows,column_n=4, row_n=3, figsize=(15,10)):\n",
    "    \"\"\"plot a grid of reconstruction plots\n",
    "\n",
    "    :param rows: Data rows (as extracted from the measurements data-frame\n",
    "    :param column_n: number of columns\n",
    "    :param row_n:  number of rows\n",
    "    :param figsize: Size of figure\n",
    "    :returns: None\n",
    "    :rtype: \n",
    "\n",
    "    \"\"\"\n",
    "    fig,axes=plt.subplots(row_n,column_n, sharex='col', sharey='row',figsize=figsize);\n",
    "    k=0\n",
    "    for i in range(row_n):\n",
    "        for j in range(column_n):\n",
    "            row=rows[k]\n",
    "            k+=1\n",
    "            #_title='%3.2f,r1=%3.2f,r2=%3.2f,r3=%3.2f'\\\n",
    "            #        %(row['res_mean'],row['res_1'],row['res_2'],row['res_3'])\n",
    "            #print i,j,_title,axes[i,j]\n",
    "            plot_decomp(row,Mean,v,fig=fig,ax=axes[i,j],interactive=False)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Different things to try\n",
    "The best/worst rows in terms of res_mean,res_1, res_2, res_3\n",
    "\n",
    "The rows with the highest lowest levels of coeff1, coeff2, coeff3, when the corresponding residue is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df4=df3.filter(df3.res_2<0.4).sort(df3.coeff_2)\n",
    "rows=df4.take(12)\n",
    "df4.select('coeff_2','res_2').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_recon_grid(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df5=df3.filter(df3.res_2<0.4).sort(df3.coeff_2,ascending=False)\n",
    "rows=df5.take(12)\n",
    "df5.select('coeff_2','res_2').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "plot_recon_grid(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interactive plot of reconstruction\n",
    "\n",
    "Following is an interactive widget which lets you change the coefficients of the eigen-vectors to see the effect on the approximation.\n",
    "The initial state of the sliders (in the middle) corresponds to the optimal setting. You can zero a positive coefficient by moving the slider all the way down, zero a negative coefficient by moving it all the way up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "row=rows[0]\n",
    "target=np.array(unpackArray(row.vector,np.float16),dtype=np.float64)\n",
    "eigen_decomp=Eigen_decomp(None,target,Mean,v)\n",
    "total_var,residuals,reductions,coeff=eigen_decomp.compute_var_explained()\n",
    "res=residuals[1]\n",
    "print 'residual normalized norm  after mean:',res[0]\n",
    "print 'residual normalized norm  after mean + top eigs:',res[1:]\n",
    "\n",
    "plotter=recon_plot(eigen_decomp,year_axis=True,interactive=True)\n",
    "display(plotter.get_Interactive())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### What is the distribution of the residuals and the coefficients?\n",
    "\n",
    "To answer this question we extract all of the values of `res_3` which is the residual variance after the Mean and the \n",
    "first two Eigen-vectors have been subtracted out. We rely here on the fact that `df3` is already sorted according to `res_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# A function for plotting the CDF of a given feature\n",
    "def plot_CDF(feat):\n",
    "    rows=df3.select(feat).sort(feat).collect()\n",
    "    vals=[r[feat] for r in rows]\n",
    "    P=np.arange(0,1,1./(len(vals)))\n",
    "    #vals=[vals[0]]+vals\n",
    "    plot(vals,P)\n",
    "    title('cumulative distribution of '+feat)\n",
    "    ylabel('number of instances')\n",
    "    xlabel(feat)\n",
    "    grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_CDF('res_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "plot_CDF('coeff_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "filename=data_dir+'/decon_'+file_index+'_'+m+'.parquet'\n",
    "!rm -rf $filename\n",
    "df3.write.parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!du -sh $data_dir/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "118px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {
    "0d4726d074414304b7910c9bc9aee2a0": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "9dfb4bbaf8664891a93b62da7476d8fe": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
