{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#setup\n",
    "data_dir='../../Data/Weather'\n",
    "file_index='BSBSSSSB'\n",
    "m='SNWD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "variables": {
     "m": "SNWD"
    }
   },
   "source": [
    "## Reconstruction using top eigen-vectors\n",
    "For measurement = {{m}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Enable automiatic reload of libraries\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2 # means that all modules are reloaded before every command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "    pandas as    pd \tversion=0.19.2 \trequired version>=0.19.2\n",
      "     numpy as    np \tversion=1.12.1 \trequired version>=1.12.0\n",
      "   sklearn as    sk \tversion=0.18.1 \trequired version>=0.18.1\n",
      "    urllib as urllib \tversion=1.17 \trequired version>=1.17\n",
      "   pyspark as pyspark \tversion=2.1.0+hadoop2.6 \trequired version>=2.1.0\n",
      "ipywidgets as ipywidgets \tversion=6.0.0 \trequired version>=6.0.0\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import sys\n",
    "sys.path.append('./lib')\n",
    "\n",
    "from numpy_pack import packArray,unpackArray\n",
    "\n",
    "from Eigen_decomp import Eigen_decomp\n",
    "from YearPlotter import YearPlotter\n",
    "from recon_plot import recon_plot\n",
    "\n",
    "from import_modules import import_modules,modules\n",
    "import_modules(modules)\n",
    "\n",
    "from ipywidgets import interactive,widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "#sc.stop()\n",
    "\n",
    "sc = SparkContext(master=\"local[3]\",pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStats.py','lib/recon_plot.py','lib/Eigen_decomp.py'])\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read Statistics File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../../Data/Weather/STAT_BSBSSSSB.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-05cf8c47d594>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#read statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/STAT_%s.pickle'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mfile_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mSTAT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSTAT_Descriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmeasurements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTAT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'keys from STAT='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmeasurements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../../Data/Weather/STAT_BSBSSSSB.pickle'"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "\n",
    "#read statistics\n",
    "filename=data_dir+'/STAT_%s.pickle'%file_index\n",
    "STAT,STAT_Descriptions = load(open(filename,'rb'))\n",
    "measurements=STAT.keys()\n",
    "print 'keys from STAT=',measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read data file into a spark DataFrame\n",
    "We focus on the snow-depth records, because the eigen-vectors for them make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "filename=data_dir+'/US_Weather_%s.parquet'%file_index\n",
    "df_in=sqlContext.read.parquet(filename)\n",
    "#filter in \n",
    "df=df_in.filter(df_in.measurement==m)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plot Reconstructions\n",
    "\n",
    "Construct approximations of a time series using the mean and the $k$ top eigen-vectors\n",
    "First, we plot the mean and the top $k$ eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "fig,axes=plt.subplots(2,1, sharex='col', sharey='row',figsize=(10,6));\n",
    "k=3\n",
    "EigVec=np.matrix(STAT[m]['eigvec'][:,:k])\n",
    "Mean=STAT[m]['Mean']\n",
    "YearPlotter().plot(Mean,fig,axes[0],label='Mean',title=m+' Mean')\n",
    "YearPlotter().plot(EigVec,fig,axes[1],title=m+' Eigs',labels=['eig'+str(i+1) for i in range(k)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "v=[np.array(EigVec[:,i]).flatten() for i in range(np.shape(EigVec)[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata