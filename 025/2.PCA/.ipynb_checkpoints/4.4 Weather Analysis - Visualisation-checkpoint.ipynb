{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Visualizing the distribution of the observations\n",
    "\n",
    "### Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "keys from STAT= ['TMIN', 'TOBS', 'TMAX', 'SNOW', 'SNWD', 'PRCP']\n",
      "12107\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "|elevation|latitude|longitude|measurement|    station|undefs|              vector|  year|   label|\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "|      3.0| 29.8667| -84.6667|       TMAX|USC00081356|     3|[C8 5A E0 54 40 5...|1900.0|BSBSSSSS|\n",
      "|      3.0| 29.8667| -84.6667|       TMAX|USC00081356|    11|[90 59 40 5A 60 5...|1901.0|BSBSSSSS|\n",
      "|      3.0| 29.8667| -84.6667|       TMAX|USC00081356|     5|[28 58 E0 58 60 5...|1902.0|BSBSSSSS|\n",
      "|      3.0| 29.8667| -84.6667|       TMAX|USC00081356|    34|[20 5B 10 5A 98 5...|1903.0|BSBSSSSS|\n",
      "|      3.0| 29.8667| -84.6667|       TMAX|USC00081356|    31|[E8 59 C8 5A E8 5...|1904.0|BSBSSSSS|\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import urllib\n",
    "import math\n",
    "%pylab inline\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "#sc.stop()\n",
    "sc = SparkContext(master=\"local[3]\",pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStats.py'])\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "import sys\n",
    "sys.path.append('./lib')\n",
    "\n",
    "import numpy as np\n",
    "from numpy_pack import packArray,unpackArray\n",
    "from spark_PCA import computeCov\n",
    "from computeStats import computeOverAllDist, STAT_Descriptions\n",
    "\n",
    "### Read the data frame from pickle file\n",
    "\n",
    "data_dir='../../Data/Weather'\n",
    "file_index='BSBSSSSS'\n",
    "\n",
    "from pickle import load\n",
    "\n",
    "#read statistics\n",
    "filename=data_dir+'/STAT_%s.pickle'%file_index\n",
    "STAT,STAT_Descriptions = load(open(filename,'rb'))\n",
    "print 'keys from STAT=',STAT.keys()\n",
    "\n",
    "#read data\n",
    "filename=data_dir+'/US_Weather_%s.parquet'%file_index\n",
    "\n",
    "df=sqlContext.read.parquet(filename)\n",
    "print df.count()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Select data for a particular station and measurement type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM weather\n",
      "\tWHERE measurement='PRCP' and station='USC00081356'\n",
      "62 rows\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "|elevation|latitude|longitude|measurement|    station|undefs|              vector|  year|   label|\n",
  