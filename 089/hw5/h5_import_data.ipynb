{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate data\n",
    "\n",
    "The data we will use here comes from [NOAA](https://www.ncdc.noaa.gov/). Specifically, it was downloaded from This [FTP site](ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/).\n",
    "\n",
    "There is a large variety of measurements from all over the world, from 1870 will 2012.\n",
    "in the directory `../../Data/Weather` you will find the following useful files:\n",
    "\n",
    "* data-source.txt: the source of the data\n",
    "* ghcnd-readme.txt: A description of the content and format of the data\n",
    "* ghcnd-stations.txt: A table describing the Meteorological stations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "* Most measurements exists only for a tiny fraction of the stations and years. We therefor restrict our use to the following measurements:\n",
    "```python\n",
    "['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS']\n",
    "```\n",
    "\n",
    "* 8 We consider only measurement-years that have at most 50 `NaN` entries\n",
    "\n",
    "* We consider only measurements in the continential USA\n",
    "\n",
    "* We partition the stations into 256 geographical rectangles, indexed from BBBBBBBB to SSSSSSSS. And each containing about 12,000 station,year pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "\n",
    "#sc.stop()\n",
    "sc = SparkContext(master=\"local[3]\",pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStats.py'])\n",
    "\n",
    "from pyspark.sql import *\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "import sys\n",
    "sys.path.append('./lib')\n",
    "\n",
    "import numpy as np\n",
    "from numpy_pack import packArray,unpackArray\n",
    "from spark_PCA import computeCov\n",
    "from computeStats import computeOverAllDist, STAT_Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_index='BBSBSBSB'\n",
    "data_dir='../../Data/Weather'\n",
    "\n",
    "filebase='US_Weather_%s'%file_index\n",
    "!rm -rf $data_dir/$filebase*\n",
    "\n",
    "c_filename=filebase+'.csv.gz'\n",
    "u_filename=filebase+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl https://mas-dse-open.s3.amazonaws.com/Weather/small/US_Weather_BBSBSBSB.csv.gz > ../../Data/Weather/US_Weather_BBSBSBSB.csv.gz\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 3790k  100 3790k    0     0  2311k      0  0:00:01  0:00:01 --:--:-- 2322k\n",
      "-rw-rw-r-- 1 kriti kriti 3.8M May 13 23:32 ../../Data/Weather/US_Weather_BBSBSBSB.csv.gz\n"
     ]
    }
   ],
   "source": [
    "command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/small/%s > %s/%s\"%(c_filename,data_dir,c_filename)\n",
    "print command\n",
    "!$command\n",
    "!ls -lh $data_dir/$c_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Data/Weather/US_Weather_BBSBSBSB.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12493"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unzip\n",
    "!gunzip -c $data_dir/$c_filename > $data_dir/$u_filename\n",
    "import pickle\n",
    "print(data_dir+'/'+u_filename)\n",
    "List=pickle.load(open(data_dir+'/'+u_filename,'rb'))\n",
    "len(List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12493\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "|elevation|latitude|longitude|measurement|    station|undefs|              vector|  year|   label|\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "|    400.8| 47.4306| -94.0586|       TMAX|USC00219059|    20|[00 7E 40 D2 00 D...|1898.0|BBSBSBSB|\n",
      "|    400.8| 47.4306| -94.0586|       TMAX|USC00219059|    31|[30 D4 80 D4 30 D...|1900.0|BBSBSBSB|\n",
      "|    400.8| 47.4306| -94.0586|       TMAX|USC00219059|    28|[00 7E 00 7E A0 D...|1901.0|BBSBSBSB|\n",
      "|    400.8| 47.4306| -94.0586|       TMAX|USC00219059|    32|[00 7E 00 7E 00 7...|1902.0|BBSBSBSB|\n",
      "|    400.8| 47.4306| -94.0586|       TMAX|USC00219059|    28|[00 00 00 D3 30 D...|1903.0|BBSBSBSB|\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=sqlContext.createDataFrame(List)\n",
    "print df.count()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#store dataframe as parquet file\n",
    "outfilename=data_dir+'/'+filebase+'.parquet'\n",
    "!rm -rf $outfilename\n",
    "df.write.save(outfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of executors= 3\n",
      "took 0.000958919525146 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t=time()\n",
    "\n",
    "N=sc.defaultParallelism\n",
    "print 'Number of executors=',N\n",
    "print 'took',time()-t,'seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measurements=['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(df,'weather') #using older sqlContext instead of newer (V2.0) sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM weather\n",
      "\tWHERE measurement = 'TMAX'\n",
      "shape of E= (365,) shape of NE= (365,)\n",
      "time for TMAX is 18.5354249477\n",
      "SELECT * FROM weather\n",
      "\tWHERE measurement = 'SNOW'\n",
      "shape of E= (365,) shape of NE= (365,)\n",
      "time for SNOW is 14.4272851944\n",
      "SELECT * FROM weather\n",
      "\tWHERE measurement = 'SNWD'\n",
      "shape of E= (365,) shape of NE= (365,)\n",
      "time for SNWD is 9.92165112495\n",
      "SELECT * FROM weather\n",
      "\tWHERE measurement = 'TMIN'\n",
      "shape of E= (365,) shape of NE= (365,)\n",
      "time for TMIN is 16.5738809109\n",
      "SELECT * FROM weather\n",
      "\tWHERE measurement = 'PRCP'\n",
      "shape of E= (365,) shape of NE= (365,)\n",
      "time for PRCP is 20.4796979427\n",
      "SELECT * FROM weather\n",
      "\tWHERE measurement = 'TOBS'\n",
      "shape of E= (365,) shape of NE= (365,)\n",
      "time for TOBS is 6.96630096436\n"
     ]
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "STAT={}  # dictionary storing the statistics for each measurement\n",
    "Clean_Tables={}\n",
    "\n",
    "for meas in measurements:\n",
    "    t=time()\n",
    "    Query=\"SELECT * FROM weather\\n\\tWHERE measurement = '%s'\"%(meas)\n",
    "    print Query\n",
    "    df = sqlContext.sql(Query)\n",
    "    data=df.rdd.map(lambda row: unpackArray(row['vector'],np.float16))\n",
    "    #get very basic statistics\n",
    "    STAT[meas]=computeOverAllDist(data)   # Compute the statistics \n",
    "\n",
    "    # compute covariance matrix\n",
    "    OUT=computeCov(data)\n",
    "\n",
    "    #find PCA decomposition\n",
    "    eigval,eigvec=LA.eig(OUT['Cov'])\n",
    "\n",
    "    # collect all of the statistics in STAT[meas]\n",
    "    STAT[meas]['eigval']=eigval\n",
    "    STAT[meas]['eigvec']=eigvec\n",
    "    STAT[meas].update(OUT)\n",
    "\n",
    "    print 'time for',meas,'is',time()-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "filename=data_dir+'/STAT_%s.pickle'%file_index\n",
    "dump((STAT,STAT_Descriptions),open(filename,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
