{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## iPyLeaflet\n",
    "[ipyleaflet](https://github.com/ellisonbg/ipyleaflet) is a bridge between jupyter notebooks and the [leaflet](http://leafletjs.com/)  javascript library for drawing maps.\n",
    "\n",
    "ipyleaflet comes with a few examples notebooks (this notebook was derived from one) but very little documentation,\n",
    "for more documentation read the [Leaflet IPA](http://leafletjs.com/reference.html)\n",
    "\n",
    "For installation directions, see the README on [ipyleaflet](https://github.com/ellisonbg/ipyleaflet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from ipyleaflet import (\n",
    "    Map,\n",
    "    Marker,\n",
    "    TileLayer, ImageOverlay,\n",
    "    Polyline, Polygon, Rectangle, Circle, CircleMarker,\n",
    "    GeoJSON,\n",
    "    DrawControl\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Visualizing the distribution of the observations\n",
    "\n",
    "## Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import urllib\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[3]) created by __init__ at <ipython-input-3-9a9c5a5dabc7>:6 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9a9c5a5dabc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#sc.stop()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"local[3]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpyFiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lib/numpy_pack.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lib/computeStats.py'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhenlian/Desktop/ComputerSoftware/Spark/spark-1.6.0-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/Users/zhenlian/Desktop/ComputerSoftware/Spark/spark-1.6.0-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    259\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 261\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    262\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[3]) created by __init__ at <ipython-input-3-9a9c5a5dabc7>:6 "
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "#sc.stop()\n",
    "sc = SparkContext(master=\"local[3]\",pyFiles=['lib/numpy_pack.py','lib/computeStats.py'])\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./lib')\n",
    "\n",
    "import numpy as np\n",
    "from numpy_pack import packArray,unpackArray\n",
    "from computeStats import computeOverAllDist, STAT_Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('keys from STAT=', ['TMIN', 'TOBS', 'TMAX', 'SNOW', 'SNWD', 'PRCP'])\n",
      "2146\n",
      "+-------------------+------------------+-------------------+---------+--------+--------+---------+-----------+-------------------+-------------------+-------------------+-------------------+-----------+-----------+------+--------------------+------+\n",
      "|            coeff_1|           coeff_2|            coeff_3|elevation|   label|latitude|longitude|measurement|              res_1|              res_2|              res_3|           res_mean|    station|  total_var|undefs|              vector|  year|\n",
      "+-------------------+------------------+-------------------+---------+--------+--------+---------+-----------+-------------------+-------------------+-------------------+-------------------+-----------+-----------+------+--------------------+------+\n",
      "|-1279.6068065826728| 95.88417930683788|-37.159380121393674|      3.0|BSBSSSBS| 24.6278| -82.8736|       TMIN|0.07562723612869544|0.07069061709076589|0.06887640687012085|0.09928085907765156|USC00082418|1.8308988E7|     3|[-24, 89, -56, 90...|1954.0|\n",
      "|-1243.6190900424936|102.14898183858504|  83.82196659183336|      3.0|BSBSSSBS| 24.6278| -82.8736|       TMIN|0.08193292730596788|0.07824839845654442|0.08200216646534411|0.10597967720153634|USC00082418|1.7770875E7|    15|[-80, 88, -112, 8...|1957.0|\n",
      "+-------------------+------------------+-------------------+---------+--------+--------+---------+-----------+-------------------+-------------------+-------------------+-------------------+-----------+-----------+------+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Read the data frame from pickle file\n",
    "\n",
    "data_dir='../../Data/Weather'\n",
    "file_index='BSBSSSBS'\n",
    "meas='TMIN'\n",
    "\n",
    "from pickle import load\n",
    "\n",
    "#read statistics\n",
    "filename=data_dir+'/STAT_%s.pickle'%file_index\n",
    "STAT,STAT_Descriptions = load(open(filename,'rb'))\n",
    "print('keys from STAT=',STAT.keys())\n",
    "\n",
    "#!ls -ld $data_dir/*.parquet\n",
    "\n",
    "#read data\n",
    "filename=data_dir+'/decon_%s_%s.parquet'%(file_index,meas)\n",
    "\n",
    "df=sqlContext.read.parquet(filename)\n",
    "print(df.count())\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT station, latitude,longitude,elevation,coeff_1 FROM weather\n",
      "+-----------+--------+---------+---------+-------------------+\n",
      "|    station|latitude|longitude|elevation|            coeff_1|\n",
      "+-----------+--------+---------+---------+-------------------+\n",
      "|USC00082418| 24.6278| -82.8736|      3.0|-1279.6068065826728|\n",
      "|USC00082418| 24.6278| -82.8736|      3.0|-1243.6190900424936|\n",
      "|USC00082418| 24.6278| -82.8736|      3.0|-1220.3240905813605|\n",
      "|USC00082418| 24.6278| -82.8736|      3.0|-1153.6304822642571|\n",
      "+-----------+--------+---------+---------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#extract longitude and latitude for each station\n",
    "feature='coeff_1'\n",
    "sqlContext.registerDataFrameAsTable(df,'weather')\n",
    "Query=\"SELECT station, latitude,longitude,elevation,%s FROM weather\"%feature\n",
    "print(Query)\n",
    "df1 = sqlContext.sql(Query)\n",
    "df1.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>count(station)</th>\n",
       "      <th>avg(coeff_1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>USC00080535</td>\n",
       "      <td>28.5500</td>\n",
       "      <td>-82.6333</td>\n",
       "      <td>14.9</td>\n",
       "      <td>8</td>\n",
       "      <td>141.200022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>USC00080598</td>\n",
       "      <td>29.7956</td>\n",
       "      <td>-82.9178</td>\n",
       "      <td>9.1</td>\n",
       "      <td>11</td>\n",
       "      <td>665.053243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        station  latitude  longitude  elevation  count(station)  avg(coeff_1)\n",
       "49  USC00080535   28.5500   -82.6333       14.9               8    141.200022\n",
       "21  USC00080598   29.7956   -82.9178        9.1              11    665.053243"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df1.groupby(['station','latitude','longitude','elevation']).agg({\"station\": \"count\", feature: \"mean\"})\n",
    "pdf=df2.toPandas()\n",
    "pdf.sort_values(by=['station'],inplace=True)\n",
    "pdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'#800000'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define a mapping from the range of the value to hex colors.\n",
    "from matplotlib.colors import rgb2hex\n",
    "_avg='avg(%s)'%feature\n",
    "_min=pdf[_avg].min()\n",
    "_max=pdf[_avg].max()\n",
    "_min,_max\n",
    "\n",
    "import pylab as plt\n",
    "cmap=plt.get_cmap('jet')\n",
    "def get_color(val):\n",
    "    x=(val-_min)/(_max-_min)\n",
    "    return(rgb2hex(cmap(x)[:3]))\n",
    "\n",
    "get_color(1000.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute \n",
    "print pdf.min()\n",
    "print pdf.max()\n",
    "print \"hey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "min_lat,max_lat,min_long,max_long = box = (42.1103, 42.6167, -72.6, -70.8)\n",
    "print \"hey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c646a63f58d466e88bfeb920056e659"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "center = [(min_lat+max_lat)/2, (min_long+max_long)/2]\n",
    "zoom = 9\n",
    "\n",
    "m = Map(default_tiles=TileLayer(opacity=1.0), center=center, zoom=zoom)\n",
    "\n",
    "r = Rectangle(bounds=[[min_lat,min_long],[max_lat,max_long]], weight=5, fill_opacity=0.0)\n",
    "m += r\n",
    "\n",
    "lat_margin=(max_lat-min_lat)/4\n",
    "long_margin=(max_long-min_long)/4\n",
    "circles = []\n",
    "for index,row in pdf.iterrows():\n",
    "    _lat=row['latitude']\n",
    "    _long=row['longitude']\n",
    "    _count=row['count(station)']\n",
    "    _coef=row[_avg]\n",
    "    # taking sqrt of count so that the  area of the circle corresponds to the count\n",
    "    c = Circle(location=(_lat,_long), radius=int(300*np.sqrt(_count+0.0)), weight=1,\n",
    "            color='#F00', opacity=0.8, fill_opacity=0.4,\n",
    "            fill_color=get_color(_coef))\n",
    "    circles.append(c)\n",
    "    m.add_layer(c)\n",
    "m    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(28.6664, -82.0894, 55), (26.5333, -82.1833, 26), (29.1333, -83.05, 63), (27.9667, -82.7667, 38), (27.9833, -82.8333, 5), (29.6333, -83.1053, 20), (27.9619, -82.5403, 74), (27.4014, -82.5586, 13), (27.9106, -82.6875, 14), (28.1878, -82.6258, 2), (27.7628, -82.6261, 100)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c646a63f58d466e88bfeb920056e659"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stations = [u'USC00081163', u'USC00081310', u'USC00081432', u'USC00081632', u'USC00081635',u'USW00012833', u'USW00012842', u'USW00012871', u'USW00012873', u'USW00092802', u'USW00092806']\n",
    "tuples = []\n",
    "for sta in stations:\n",
    "    temp_row =  pdf.loc[pdf['station'] == sta]\n",
    "    temp_row = temp_row.values[0]\n",
    "    #print temp_row\n",
    "    lat = temp_row[1]\n",
    "    lon = temp_row[2]\n",
    "    cou = temp_row[4]\n",
    "    tuples.append((lat,lon,cou))\n",
    "circles_my = []\n",
    "for (lat,lon,cou) in tuples:\n",
    "    _lat=lat\n",
    "    _long=lon\n",
    "    _count=cou\n",
    "    _coef=row[_avg]\n",
    "    # taking sqrt of count so that the  area of the circle corresponds to the count\n",
    "    c = Circle(location=(_lat,_long), radius=int(300*np.sqrt(_count+0.0)), weight=1,\n",
    "            color='#0C0', opacity=0.8, fill_opacity=0.4,\n",
    "            fill_color=get_color(_coef))\n",
    "    circles_my.append(c)\n",
    "    m.add_layer(c)\n",
    "print tuples\n",
    "m\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### excercises:\n",
    "* Add a legend that relates the colors to values.\n",
    "* Leaflet supports a variety of maps. See if you can get a topographical map as the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pdf.plot.scatter(x='elevation',y='avg(coeff_1)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RDD1=sc.parallelize([\"spark  basics\", \"spark big  data analysis\", \"spring\"]) \n",
    "RDD2=sc.parallelize([\"spark using pyspark\", \"big data\"])\n",
    " \n",
    "RDD1.subtract(RDD2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "174px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
