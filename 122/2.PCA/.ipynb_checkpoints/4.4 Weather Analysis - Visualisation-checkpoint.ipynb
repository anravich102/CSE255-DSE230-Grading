{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Visualizing the distribution of the observations\n",
    "\n",
    "### Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "keys from STAT= ['TMIN', 'TOBS', 'TMAX', 'SNOW', 'SNWD', 'PRCP']\n",
      "12741\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "|elevation|latitude|longitude|measurement|    station|undefs|              vector|  year|   label|\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "|    402.9| 36.4708|-121.1472|       TMAX|USR0000CPNN|     2|[B8 59 00 58 38 5...|2002.0|SSSBSSSS|\n",
      "|    402.9| 36.4708|-121.1472|       TMAX|USR0000CPNN|     2|[90 59 10 5A 20 5...|2003.0|SSSBSSSS|\n",
      "|    402.9| 36.4708|-121.1472|       TMAX|USR0000CPNN|     1|[E0 55 00 58 E0 5...|2004.0|SSSBSSSS|\n",
      "|    402.9| 36.4708|-121.1472|       TMAX|USR0000CPNN|     2|[50 57 90 55 A0 5...|2005.0|SSSBSSSS|\n",
      "|    402.9| 36.4708|-121.1472|       TMAX|USR0000CPNN|     7|[A0 56 F0 56 A0 5...|2006.0|SSSBSSSS|\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import urllib\n",
    "import math\n",
    "%pylab inline\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "#sc.stop()\n",
    "sc = SparkContext(master=\"local[3]\",pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStats.py'])\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "import sys\n",
    "sys.path.append('./lib')\n",
    "\n",
    "import numpy as np\n",
    "from numpy_pack import packArray,unpackArray\n",
    "from spark_PCA import computeCov\n",
    "from computeStats import computeOverAllDist, STAT_Descriptions\n",
    "\n",
    "### Read the data frame from pickle file\n",
    "\n",
    "data_dir='../../Data/Weather'\n",
    "file_index='SSSBSSSS'\n",
    "\n",
    "from pickle import load\n",
    "\n",
    "#read statistics\n",
    "filename=data_dir+'/STAT_%s.pickle'%file_index\n",
    "STAT,STAT_Descriptions = load(open(filename,'rb'))\n",
    "print 'keys from STAT=',STAT.keys()\n",
    "\n",
    "#read data\n",
    "filename=data_dir+'/US_Weather_%s.parquet'%file_index\n",
    "\n",
    "df=sqlContext.read.parquet(filename)\n",
    "print df.count()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Select data for a particular station and measurement type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM weather\n",
      "\tWHERE measurement='TMAX' and station='USR0000CPNN'\n",
      "9 rows\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "|elevation|latitude|longitude|measurement|    station|undefs|              vector|  year|   label|\n",
   