{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#setup\n",
    "data_dir='../../Data/Weather'\n",
    "file_index='BSBSSSBB'#BSBSSSBB\n",
    "m='SNWD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "variables": {
     "m": "SNWD"
    }
   },
   "source": [
    "## Reconstruction using top eigen-vectors\n",
    "For measurement = {{m}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Enable automiatic reload of libraries\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2 # means that all modules are reloaded before every command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import sys\n",
    "sys.path.append('./lib')\n",
    "\n",
    "from numpy_pack import packArray,unpackArray\n",
    "\n",
    "from Eigen_decomp import Eigen_decomp\n",
    "from YearPlotter import YearPlotter\n",
    "from recon_plot import recon_plot\n",
    "\n",
    "from import_modules import import_modules,modules\n",
    "import_modules(modules)\n",
    "\n",
    "from ipywidgets import interactive,widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(master=\"local[3]\",pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStats.py','lib/recon_plot.py','lib/Eigen_decomp.py'])\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read Statistics File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "#read statistics\n",
    "filename=data_dir+'/STAT_%s.pickle'%file_index\n",
    "STAT,STAT_Descriptions = load(open(filename,'rb'))\n",
    "measurements=STAT.keys()\n",
    "print 'keys from STAT=',measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read data file into a spark DataFrame\n",
    "We focus on the snow-depth records, because the eigen-vectors for them make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "filename=data_dir+'/US_Weather_%s.parquet'%file_index\n",
    "df_in=sqlContext.read.parquet(filename)\n",
    "#filter in \n",
    "df=df_in.filter(df_in.measurement==m)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plot Reconstructions\n",
    "\n",
    "Construct approximations of a time series using the mean and the $k$ top eigen-vectors\n",
    "First, we plot the mean and the top $k$ eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "fig,axes=plt.subplots(2,1, sharex='col', sharey='row',figsize=(10,6));\n",
    "k=5\n",
    "EigVec=np.matrix(STAT[m]['eigvec'][:,:k])\n",
    "Mean=STAT[m]['Mean']\n",
    "YearPlotter().plot(Mean,fig,axes[0],label='Mean',title=m+' Mean')\n",
    "YearPlotter().plot(EigVec,fig,axes[1],title=m+' Eigs',labels=['eig'+str(i+1) for i in range(k)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "v=[np.array(EigVec[:,i]).flatten() for i in range(np.shape(EigVec)[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### plot the percent of residual variance on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  x=0 in the graphs below correspond to the fraction of the variance explained by the mean alone\n",
    "#  x=1,2,3,... are the residuals for eig1, eig1+eig2, eig1+eig2+eig3 ...\n",
    "fig,ax=plt.subplots(1,1);\n",
    "eigvals=STAT[m]['eigval']; eigvals/=sum(eigvals); cumvar=np.cumsum(eigvals); cumvar=100*np.insert(cumvar,0,0)\n",
    "ax.plot(cumvar[:10]); \n",
    "ax.grid(); \n",
    "ax.set_ylabel('Percent of variance explained')\n",
    "ax.set_xlabel('number of eigenvectors')\n",
    "ax.set_title('Percent of variance explained');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Process whole dataframe to find best and worse residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Add to each row in the dataframe a residual values \n",
    "Residuals are after subtracting in sequence: the mean, the projection on the first eigen-vector the projection on the second eigen-vector etc.\n",
    "\n",
    "`decompose(row)` axtracts the series from the row, computes the residuals and constructs a new row that is reassembled into a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def decompose(row):\n",
    "    \"\"\"compute residual and coefficients for decomposition           \n",
    "\n",
    "    :param row: SparkSQL Row that contains the measurements for a particular station, year and measurement. \n",
    "    :returns: the input row with additional information from the eigen-decomposition.\n",
    "    :rtype: SparkSQL Row \n",
    "\n",
    "    Note that Decompose is designed to run inside a spark \"map()\" command.\n",
    "    Mean and v are sent to the workers as local variables of \"Decompose\"\n",
    "\n",
    "    \"\"\"\n",
    "    Series=np.array(unpackArray(row.vector,np.float16),dtype=np.float64)\n",
    "    recon=Eigen_decomp(None,Series,Mean,v);\n",
    "    total_var,residuals,reductions,coeff=recon.compute_var_explained()\n",
    "    #print coeff\n",
    "    residuals=[float(r) for r in residuals[1]]\n",
    "    coeff=[float(r) for r in coeff[1]]\n",
    "    D=row.asDict()\n",
    "    D['total_var']=float(total_var[1])\n",
    "    D['res_mean']=residuals[0]\n",
    "    for i in range(1,len(residuals)):\n",
    "        D['res_'+str(i)]=residuals[i]\n",
    "        D['coeff_'+str(i)]=coeff[i-1]\n",
    "    return Row(**D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "rdd2=df.rdd.map(decompose)\n",
    "df2=sqlContext.createDataFrame(rdd2)\n",
    "row,=df2.take(1)\n",
    "\n",
    "#filter out vectors for which the mean is a worse approximation than zero.\n",
    "print 'before filter',df2.count()\n",
    "df3=df2.filter(df2.res_mean<1)\n",
    "print 'after filter',df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Sort entries by increasing values of ers_3\n",
    "df3=df3.sort(df3.res_2,ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_decomp(row,Mean,v,fig=None,ax=None,Title=None,interactive=False):\n",
    "    \"\"\"Plot a single reconstruction with an informative title\n",
    "\n",
    "    :param row: SparkSQL Row that contains the measurements for a particular station, year and measurement. \n",
    "    :param Mean: The mean vector of all measurements of a given type\n",
    "    :param v: eigen-vectors for the distribution of measurements.\n",
    "    :param fig: a matplotlib figure in which to place the plot\n",
    "    :param ax: a matplotlib axis in which to place the plot\n",
    "    :param Title: A plot title over-ride.\n",
    "    :param interactive: A flag that indicates whether or not this is an interactive plot (widget-driven)\n",
    "    :returns: a plotter returned by recon_plot initialization\n",
    "    :rtype: recon_plot\n",
    "\n",
    "    \"\"\"\n",
    "    target=np.array(unpackArray(row.vector,np.float16),dtype=np.float64)\n",
    "    if Title is None:\n",
    "        Title='%s / %d    %s'%(row['station'],row['year'],row['measurement'])\n",
    "    eigen_decomp=Eigen_decomp(range(1,366),target,Mean,v)\n",
    "    plotter=recon_plot(eigen_decomp,year_axis=True,fig=fig,ax=ax,interactive=interactive,Title=Title)\n",
    "    return plotter\n",
    "\n",
    "def plot_recon_grid(rows,column_n=3, row_n=3, figsize=(15,10), coef='coeff_1'):\n",
    "    \"\"\"plot a grid of reconstruction plots\n",
    "\n",
    "    :param rows: Data rows (as extracted from the measurements data-frame\n",
    "    :param column_n: number of columns\n",
    "    :param row_n:  number of rows\n",
    "    :param figsize: Size of figure\n",
    "    :returns: None\n",
    "    :rtype: \n",
    "\n",
    "    \"\"\"\n",
    "    fig,axes=plt.subplots(row_n,column_n, sharex='col', sharey='row',figsize=figsize);\n",
    "    k=0\n",
    "    for i in range(row_n):\n",
    "        for j in range(column_n):\n",
    "            if (len(rows) > k):\n",
    "                row=rows[k]\n",
    "                k+=1\n",
    "                _title='%s=%3.2f,r1=%3.2f,r2=%3.2f,r3=%3.2f'\\\n",
    "                    %(coef,row[coef],row['res_1'],row['res_2'],row['res_3'])\n",
    "            #print i,j,_title,axes[i,j]\n",
    "                plot_decomp(row,Mean,v,fig=fig,ax=axes[i,j],interactive=False, Title=_title)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Different things to try\n",
    "The best/worst rows in terms of res_mean,res_1, res_2, res_3\n",
    "\n",
    "The rows with the highest lowest levels of coeff1, coeff2, coeff3, when the corresponding residue is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#df4=df3.filter(df3.res_2<0.4).sort(df3.coeff_2)\n",
    "#rows=df4.take(12)\n",
    "#df4.select('coeff_2','res_2').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_recon_grid(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df5=df3.filter(df3.res_2<0.4).sort(df3.coeff_2,ascending=False)\n",
    "#rows=df5.take(12)\n",
    "#df5.select('coeff_2','res_2').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#plot_recon_grid(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interactive plot of reconstruction\n",
    "\n",
    "Following is an interactive widget which lets you change the coefficients of the eigen-vectors to see the effect on the approximation.\n",
    "The initial state of the sliders (in the middle) corresponds to the optimal setting. You can zero a positive coefficient by moving the slider all the way down, zero a negative coefficient by moving it all the way up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "row=rows[0]\n",
    "target=np.array(unpackArray(row.vector,np.float16),dtype=np.float64)\n",
    "eigen_decomp=Eigen_decomp(None,target,Mean,v)\n",
    "total_var,residuals,reductions,coeff=eigen_decomp.compute_var_explained()\n",
    "res=residuals[1]\n",
    "print 'residual normalized norm  after mean:',res[0]\n",
    "print 'residual normalized norm  after mean + top eigs:',res[1:]\n",
    "\n",
    "plotter=recon_plot(eigen_decomp,year_axis=True,interactive=True)\n",
    "display(plotter.get_Interactive())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### What is the distribution of the residuals and the coefficients?\n",
    "\n",
    "To answer this question we extract all of the values of `res_3` which is the residual variance after the Mean and the \n",
    "first two Eigen-vectors have been subtracted out. We rely here on the fact that `df3` is already sorted according to `res_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# A function for plotting the CDF of a given feature\n",
    "def plot_CDF(feat):\n",
    "    rows=df3.select(feat).sort(feat).collect()\n",
    "    vals=[r[feat] for r in rows]\n",
    "    P=np.arange(0,(1+1./(len(vals))),1./(len(vals)))\n",
    "    vals=[vals[0]]+vals\n",
    "    plot(vals,P)\n",
    "    title('cumulative distribution of '+feat)\n",
    "    ylabel('number of instances')\n",
    "    xlabel(feat)\n",
    "    grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_CDF('res_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "plot_CDF('coeff_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "filename=data_dir+'/decon_'+file_index+'_'+m+'.parquet'\n",
    "!rm -rf $filename\n",
    "df3.write.parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!du -sh $data_dir/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipyleaflet import (\n",
    "    Map,\n",
    "    Marker,\n",
    "    TileLayer, ImageOverlay,\n",
    "    Polyline, Polygon, Rectangle, Circle, CircleMarker,\n",
    "    GeoJSON,\n",
    "    DrawControl\n",
    ")\n",
    "\n",
    "ab_list = []\n",
    "pdf = df4.toPandas()\n",
    "min_lat = np.min(pdf['latitude'])\n",
    "max_lat = np.max(pdf['latitude'])\n",
    "min_long = np.min(pdf['longitude'])\n",
    "max_long = np.max(pdf['longitude'])\n",
    "\n",
    "\n",
    "for ii in ['coeff_1','coeff_2','coeff_3','coeff_4']:\n",
    "\n",
    "    tab_list.append(df3.sort(ii, ascending=False).limit(4).toPandas())\n",
    "    tab_list.append(df3.sort(ii, ascending=True).limit(4).toPandas())\n",
    "    \n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "\n",
    "def ipyplotter_2(table_list):\n",
    "    #min_lat,max_lat,min_long,max_long = box = (39.1014, 39.8227, -108.7339, -105.1171)\n",
    "    center = [(min_lat+max_lat)/2, (min_long+max_long)/2]\n",
    "\n",
    "    center = [(min_lat+max_lat)/2, (min_long+max_long)/2]\n",
    "    zoom = 9\n",
    "\n",
    "    m = Map(default_tiles=TileLayer(opacity=1.0), center=center, zoom=zoom)\n",
    "\n",
    "    r = Rectangle(bounds=[[min_lat,min_long],[max_lat,max_long]], weight=5, fill_opacity=0.0)\n",
    "    m += r\n",
    "\n",
    "    lat_margin=(max_lat-min_lat)/4\n",
    "    long_margin=(max_long-min_long)/4\n",
    "    circles = []\n",
    "    for iii in range(len(table_list)):\n",
    "        for index,row in table_list[iii].iterrows():\n",
    "            _lat=row['latitude']\n",
    "            _long=row['longitude']\n",
    "            c = Circle(location=(_lat,_long), radius=1200, weight=1,\n",
    "                    color=colors[iii], opacity=0.8, fill_opacity=0.8,\n",
    "                    fill_color=colors[iii])\n",
    "            circles.append(c)\n",
    "            m.add_layer(c)\n",
    "        \n",
    "        \n",
    "    return m\n",
    "\n",
    "\n",
    "ipyplotter_2(tab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "118px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
