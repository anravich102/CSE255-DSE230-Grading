{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## iPyLeaflet\n",
    "[ipyleaflet](https://github.com/ellisonbg/ipyleaflet) is a bridge between jupyter notebooks and the [leaflet](http://leafletjs.com/)  javascript library for drawing maps.\n",
    "\n",
    "ipyleaflet comes with a few examples notebooks (this notebook was derived from one) but very little documentation,\n",
    "for more documentation read the [Leaflet IPA](http://leafletjs.com/reference.html)\n",
    "\n",
    "For installation directions, see the README on [ipyleaflet](https://github.com/ellisonbg/ipyleaflet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from ipyleaflet import (\n",
    "    Map,\n",
    "    Marker,\n",
    "    TileLayer, ImageOverlay,\n",
    "    Polyline, Polygon, Rectangle, Circle, CircleMarker,\n",
    "    GeoJSON,\n",
    "    DrawControl\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Visualizing the distribution of the observations\n",
    "\n",
    "## Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import urllib\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "#sc.stop()\n",
    "sc = SparkContext(master=\"local[3]\",pyFiles=['lib/numpy_pack.py','lib/computeStats.py'])\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./lib')\n",
    "\n",
    "import numpy as np\n",
    "from numpy_pack import packArray,unpackArray\n",
    "from computeStats import computeOverAllDist, STAT_Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('keys from STAT=', ['TMIN', 'TOBS', 'TMAX', 'SNOW', 'SNWD', 'PRCP'])\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "u'Path does not exist: file:/CSE255-DSE230/Data/Weather/decon_BSBSSSBB_TOBS.parquet;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-2cdc7292d1d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/decon_%s_%s.parquet'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmeas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \"\"\"\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Path does not exist: file:/CSE255-DSE230/Data/Weather/decon_BSBSSSBB_TOBS.parquet;'"
     ]
    }
   ],
   "source": [
    "### Read the data frame from pickle file\n",
    "\n",
    "data_dir='../../Data/Weather'\n",
    "file_index='BSBSSSBB'#BBBSBBBB'\n",
    "meas='TOBS'# 'SNWD'\n",
    "\n",
    "from pickle import load\n",
    "\n",
    "#read statistics\n",
    "filename=data_dir+'/STAT_%s.pickle'%file_index\n",
    "STAT,STAT_Descriptions = load(open(filename,'rb'))\n",
    "print('keys from STAT=',STAT.keys())\n",
    "\n",
    "#!ls -ld $data_dir/*.parquet\n",
    "\n",
    "#read data\n",
    "filename=data_dir+'/decon_%s_%s.parquet'%(file_index,meas)\n",
    "\n",
    "df=sqlContext.read.parquet(filename)\n",
    "print(df.count())\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(df,'weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT elevation, vector, station, latitude, longitude FROM weather\n",
      "\t\n",
      "109 rows\n",
      "+---------+--------------------+-----------+--------+---------+\n",
      "|elevation|              vector|    station|latitude|longitude|\n",
      "+---------+--------------------+-----------+--------+---------+\n",
      "|     44.2|[00 00 00 00 00 0...|USC00099186| 31.2514| -82.3128|\n",
      "|     72.5|[00 00 00 7E 00 0...|USC00096237| 31.1333| -83.2167|\n",
      "|     80.8|[00 00 00 00 00 0...|USC00098974| 30.8056| -83.2736|\n",
      "|     44.8|[00 00 00 00 00 0...|USC00084394| 30.5228| -82.9447|\n",
      "|     79.2|[00 00 00 00 00 0...|USC00098351| 32.4414| -82.2181|\n",
      "+---------+--------------------+-----------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "(109, 5)\n"
     ]
    }
   ],
   "source": [
    "# Plot the high and low snow density areas\n",
    "\n",
    "Query=\"SELECT elevation, vector, station, latitude, longitude FROM weather\\n\\t\"\n",
    "print Query\n",
    "df1 = sqlContext.sql(Query)\n",
    "print df1.count(),'rows'\n",
    "df1.show(5)\n",
    "rows=df1.rdd.map(lambda row:[unpackArray(row['vector'],np.float16), row['elevation'],row['station'],row['latitude'],row['longitude']]).collect()\n",
    "print (len(rows), len(rows[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector</th>\n",
       "      <th>elevation</th>\n",
       "      <th>station</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>44.2</td>\n",
       "      <td>USC00099186</td>\n",
       "      <td>31.2514</td>\n",
       "      <td>-82.3128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>72.5</td>\n",
       "      <td>USC00096237</td>\n",
       "      <td>31.1333</td>\n",
       "      <td>-83.2167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>80.8</td>\n",
       "      <td>USC00098974</td>\n",
       "      <td>30.8056</td>\n",
       "      <td>-83.2736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>44.8</td>\n",
       "      <td>USC00084394</td>\n",
       "      <td>30.5228</td>\n",
       "      <td>-82.9447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>79.2</td>\n",
       "      <td>USC00098351</td>\n",
       "      <td>32.4414</td>\n",
       "      <td>-82.2181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              vector  elevation      station  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       44.2  USC00099186   \n",
       "1  [0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       72.5  USC00096237   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       80.8  USC00098974   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       44.8  USC00084394   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       79.2  USC00098351   \n",
       "\n",
       "   latitude  longitude  \n",
       "0   31.2514   -82.3128  \n",
       "1   31.1333   -83.2167  \n",
       "2   30.8056   -83.2736  \n",
       "3   30.5228   -82.9447  \n",
       "4   32.4414   -82.2181  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df = pd.DataFrame(rows)\n",
    "pd_df.columns = ['vector','elevation','station','latitude','longitude']\n",
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_col = [np.nansum(x) for x in pd_df['vector']]\n",
    "len(new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_df['vecSum'] = new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector</th>\n",
       "      <th>elevation</th>\n",
       "      <th>station</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>vecSum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>70.1</td>\n",
       "      <td>USC00092839</td>\n",
       "      <td>32.5575</td>\n",
       "      <td>-82.9036</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>97.5</td>\n",
       "      <td>USC00098496</td>\n",
       "      <td>32.5806</td>\n",
       "      <td>-82.3822</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>53.3</td>\n",
       "      <td>USC00095858</td>\n",
       "      <td>32.8089</td>\n",
       "      <td>-82.2372</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>USC00096838</td>\n",
       "      <td>31.3781</td>\n",
       "      <td>-82.1292</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>61.0</td>\n",
       "      <td>USC00098476</td>\n",
       "      <td>31.7078</td>\n",
       "      <td>-82.1969</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               vector  elevation      station  \\\n",
       "61  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       70.1  USC00092839   \n",
       "46  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       97.5  USC00098496   \n",
       "28  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       53.3  USC00095858   \n",
       "20  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       32.0  USC00096838   \n",
       "75  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       61.0  USC00098476   \n",
       "\n",
       "    latitude  longitude  vecSum  \n",
       "61   32.5575   -82.9036    51.0  \n",
       "46   32.5806   -82.3822    51.0  \n",
       "28   32.8089   -82.2372    51.0  \n",
       "20   31.3781   -82.1292    51.0  \n",
       "75   31.7078   -82.1969    51.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  51.,   25.,   64.,   13.,   50.,   38.,   89.,   92.,   21.,\n",
       "        152.,  110.,   35.,    8.,   18.,   56.,   81.,  179.,  102.,\n",
       "         69.,   33.,   63.,   99.,   40.,   87.,  191.,   75.,   41.,\n",
       "         30.,   82.,   46.,   79.,   76.,  101.,  355.,   26.,   84.,\n",
       "         53.,  127.,  114.,  119.,   58.,   23.,    5.,    3.,  112.,\n",
       "         20.,   45.,   71.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df['vecSum'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'#000080'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define a mapping from the range of the value to hex colors.\n",
    "from matplotlib.colors import rgb2hex\n",
    "_avg='vecSum'\n",
    "_min=pd_df[_avg].min()\n",
    "_max=pd_df[_avg].max()\n",
    "_min,_max\n",
    "\n",
    "import pylab as plt\n",
    "cmap=plt.get_cmap('jet')\n",
    "def get_color(val):\n",
    "    x=(val-_min)/(_max-_min)\n",
    "    return(rgb2hex(cmap(x)[:3]))\n",
    "\n",
    "get_color(1000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "min_lat,max_lat,min_long,max_long = box = (np.min(pd_df['latitude']), np.max(pd_df['latitude']), np.min(pd_df['longitude']), np.max(pd_df['longitude']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c213dc550d45889f8d992a11d9a459"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "center = [(min_lat+max_lat)/2, (min_long+max_long)/2]\n",
    "zoom = 9\n",
    "\n",
    "m = Map(default_tiles=TileLayer(opacity=1.0), center=center, zoom=zoom)\n",
    "\n",
    "r = Rectangle(bounds=[[min_lat,min_long],[max_lat,max_long]], weight=5, fill_opacity=0.0)\n",
    "m += r\n",
    "\n",
    "lat_margin=(max_lat-min_lat)/4\n",
    "long_margin=(max_long-min_long)/4\n",
    "circles = []\n",
    "\n",
    "pd_df.sort_values(by='vecSum', inplace=True,ascending=False)\n",
    "count = 0\n",
    "for index,row in pd_df.iterrows():#pdf\n",
    "    _lat=row['latitude']\n",
    "    _long=row['longitude']\n",
    "    _count=(row['vecSum']/100) #row['count(station)']\n",
    "    if (_count > 0): #vary\n",
    "        _coef=5 #row[_avg]\n",
    "        # taking sqrt of count so that the  area of the circle corresponds to the count\n",
    "        c = Circle(location=(_lat,_long), radius=int(300*np.sqrt(_count+0.0)), weight=1,\n",
    "            color='#F00', opacity=0.8, fill_opacity=0.4,\n",
    "            fill_color=get_color(_coef))\n",
    "        circles.append(c)\n",
    "        m.add_layer(c)\n",
    "        count += 1\n",
    "        if (count > 50):\n",
    "            break\n",
    "m    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### excercises:\n",
    "* Add a legend that relates the colors to values.\n",
    "* Leaflet supports a variety of maps. See if you can get a topographical map as the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pdf.plot.scatter(x='elevation',y='avg(coeff_1)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "174px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
