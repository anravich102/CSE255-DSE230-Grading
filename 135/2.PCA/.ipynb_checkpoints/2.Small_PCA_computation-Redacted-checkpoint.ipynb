{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computing PCA using RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  PCA\n",
    "\n",
    "The vectors that we want to analyze have length, or dimension, of 365, corresponding to the number of \n",
    "days in a year.\n",
    "\n",
    "We want to perform [Principle component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "on these vectors. There are two steps to this process:\n",
    "\n",
    "1. Computing the covariance matrix: this is a  simple computation. However, it takes a long time to compute and it benefits from using an RDD because it involves all of the input vectors.\n",
    "2. Computing the eigenvector decomposition. this is a more complex computation, but it takes a fraction of a second because the size to the covariance matrix is $365 \\times 365$, which is quite small. We do it on the head node usin `linalg`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computing the covariance matrix\n",
    "Suppose that the data vectors are the column vectors denoted $x$ then the covariance matrix is defined to be\n",
    "$$\n",
    "E(x x^T)-E(x)E(x)^T\n",
    "$$\n",
    "\n",
    "Where $x x^T$ is the **outer product** of $x$ with itself.\n",
    "\n",
    "If the data that we have is $x_1,x_2,x_n$ then the estimates we use are:\n",
    "$$\n",
    "\\hat{E}(x x^T) = \\frac{1}{n} \\sum_{i=1}^n x_i x_i^T,\\;\\;\\;\\;\\;\n",
    "\\hat{E}(x) = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `nan`s in arithmetic operations\n",
    "* We store all of the measurements as single `bytearray` in a single column. Instead of using 365 columns.\n",
    "* Why?\n",
    "  1. Because serializing and desirializing is faster that way.\n",
    "  1. Because numpy treats `nan` entries correctly:\n",
    "      * In `numpy.nansum` `5+nan=5` while in dataframes `5+nan=nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of [1 1 1 2] = 1.25\n",
      "nanmean of [1 1 1 2] = 1.25\n",
      "mean of [  1.   1.  nan   2.] = nan\n",
      "nanmean of [  1.   1.  nan   2.] = 1.33333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X=np.array([1,1,1,2])\n",
    "print 'mean of',X,'=',np.mean(X)\n",
    "print 'nanmean of',X,'=',np.nanmean(X)\n",
    "X=np.array([1,1,np.NaN,2])\n",
    "print 'mean of',X,'=',np.mean(X)\n",
    "print 'nanmean of',X,'=',np.nanmean(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### When should you not use `np.nanmean` ?\n",
    "Using `n.nanmean` is equivalent to assuming that choice of which elements to remove is independent of the values of the elements. \n",
    "* Example of bad case: suppose the larger elements have a higher probability of being `nan`. In that case `np.nanmean` will under-estimate the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Computing Cov matrix on vectors with NaNs\n",
    "As it happens, we often get vectors $x$ in which some, but not all, of the entries are `nan`. \n",
    "Suppose that we want to compute the mean of the elements of $x$. If we use `np.mean` we will get the result `nan`. A useful alternative is to use `np.nanmean` which removes the `nan` elements and takes the mean of the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Computing the covariance  when there are `nan`s\n",
    "The covariance is a mean of outer products.\n",
    "\n",
    "If the data that we have is $x_1,x_2,x_n$ then the estimates we use are:\n",
    "$$\n",
    "\\hat{E}(x x^T) = \\frac{1}{n} \\sum_{i=1}^n x_i x_i^T,\\;\\;\\;\\;\\;\n",
    "\\hat{E}(x) = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  1.,  nan,   3.,   4.,   5.],\n",
       "        [ nan,  nan,  nan,  nan,  nan],\n",
       "        [  3.,  nan,   9.,  12.,  15.],\n",
       "        [  4.,  nan,  12.,  16.,  20.],\n",
       "        [  5.,  nan,  15.,  20.,  25.]],\n",
       "\n",
       "       [[  4.,   6.,   8.,  nan,  12.],\n",
       "        [  6.,   9.,  12.,  nan,  18.],\n",
       "        [  8.,  12.,  16.,  nan,  24.],\n",
       "        [ nan,  nan,  nan,  nan,  nan],\n",
       "        [ 12.,  18.,  24.,  nan,  36.]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1=np.array([1,np.NaN,3,4,5])\n",
    "x2=np.array([2,3,4,np.NaN,6])\n",
    "stacked=np.array([np.outer(x1,x1),np.outer(x2,x2)])\n",
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:1: RuntimeWarning: Mean of empty slice\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  2.5,   6. ,   5.5,   4. ,   8.5],\n",
       "       [  6. ,   9. ,  12. ,   nan,  18. ],\n",
       "       [  5.5,  12. ,  12.5,  12. ,  19.5],\n",
       "       [  4. ,   nan,  12. ,  16. ,  20. ],\n",
       "       [  8.5,  18. ,  19.5,  20. ,  30.5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(stacked,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading Data into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "#sc.stop()\n",
    "## add after creating spark_PCA.py\n",
    "sc = SparkContext(master=\"local[3]\",pyFiles=['lib/numpy_pack.py','lib/computeStats.py']) #,'lib/spark_PCA.py'\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./lib')\n",
    "\n",
    "import numpy as np\n",
    "from numpy_pack import packArray,unpackArray\n",
    "# add after creating spark_PCA.py\n",
    "# from spark_PCA import computeCov\n",
    "from computeStats import computeOverAllDist, STAT_Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Climate data\n",
    "\n",
    "The data we will use here comes from [NOAA](https://www.ncdc.noaa.gov/). Specifically, it was downloaded from This [FTP site](ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/).\n",
    "\n",
    "There is a large variety of measurements from all over the world, from 1870 will 2012.\n",
    "in the directory `../../Data/Weather` you will find the following useful files:\n",
    "\n",
    "* data-source.txt: the source of the data\n",
    "* ghcnd-readme.txt: A description of the content and format of the data\n",
    "* ghcnd-stations.txt: A table describing the Meteorological stations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data cleaning\n",
    "\n",
    "* Most measurements exists only for a tiny fraction of the stations and years. We therefor restrict our use to the following measurements:\n",
    "```python\n",
    "['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS']\n",
    "```\n",
    "\n",
    "* 8 We consider only measurement-years that have at most 50 `NaN` entries\n",
    "\n",
    "* We consider only measurements in the continential USA\n",
    "\n",
    "* We partition the stations into 256 geographical rectangles, indexed from BBBBBBBB to SSSSSSSS. And each containing about 12,000 station,year pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Data/Weather/US_Weather_BBBSBBBB.csv*\n",
      "curl https://mas-dse-open.s3.amazonaws.com/Weather/small/US_Weather_BBBSBBBB.csv.gz > ../../Data/Weather/US_Weather_BBBSBBBB.csv.gz\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 3732k  100 3732k    0     0  1536k      0  0:00:02  0:00:02 --:--:-- 1536k\n",
      "-rw-r--r--  1 rajatsharma  staff   3.6M May  7 20:17 ../../Data/Weather/US_Weather_BBBSBBBB.csv.gz\n"
     ]
    }
   ],
   "source": [
    "file_index='BBBSBBBB'\n",
    "filename='US_Weather_%s.csv.gz'%file_index\n",
    "data_dir='../../Data/Weather'\n",
    "del_files='%s/%s*'%(data_dir,filename[:-3])\n",
    "print del_files\n",
    "!rm $del_files\n",
    "command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/small/%s > %s/%s\"%(filename,data_dir,filename)\n",
    "print command\n",
    "!$command\n",
    "!ls -lh $data_dir/$filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 rajatsharma  staff    13M May  7 20:17 ../../Data/Weather/US_Weather_BBBSBBBB.csv\r\n"
     ]
    }
   ],
   "source": [
    "!gunzip $data_dir/$filename\n",
    "filename=data_dir+'/US_Weather_BBBSBBBB.csv'\n",
    "!ls -lh $filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13047"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "List=pickle.load(open(filename,'rb'))\n",
    "len(List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13047\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "|elevation|latitude|longitude|measurement|    station|undefs|              vector|  year|   label|\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "|      6.1| 42.4667|    -70.9|       TMAX|USC00198301|     0|[80 4D 00 46 E0 5...|1931.0|BBBSBBBB|\n",
      "|      6.1| 42.4667|    -70.9|       TMAX|USC00198301|     0|[00 4F 80 51 00 4...|1932.0|BBBSBBBB|\n",
      "|      6.1| 42.4667|    -70.9|       TMAX|USC00198301|     2|[20 D0 00 53 A0 5...|1933.0|BBBSBBBB|\n",
      "|      6.1| 42.4667|    -70.9|       TMAX|USC00198301|     2|[30 55 80 C9 A0 D...|1934.0|BBBSBBBB|\n",
      "|      6.1| 42.4667|    -70.9|       TMAX|USC00198301|     1|[E0 50 00 CF 00 5...|1935.0|BBBSBBBB|\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=sqlContext.createDataFrame(List)\n",
    "print df.count()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(df,'weather') #using older sqlContext instead of newer (V2.0) sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of executors= 3\n",
      "took 0.00128698348999 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t=time()\n",
    "\n",
    "N=sc.defaultParallelism\n",
    "print 'Number of executors=',N\n",
    "print 'took',time()-t,'seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computing PCA for each measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "measurements=['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework\n",
    "The code below computes the covariance matrix using RDDs.\n",
    "The code allows undefined entries and calculates the covariance without bias.\n",
    "\n",
    "Your homework is to complete the missing parts in the code (Marked with `...`) so that it would calculate the covariance correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#%%writefile lib/spark_PCA.py #once this works correctly, you should add it to the `lib` directory\n",
    "# and use import\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def outerProduct(X):\n",
    "    \"\"\"Computer outer product and indicate which locations in matrix are undefined\"\"\"\n",
    "    O=np.outer(X,X)\n",
    "    N=1-np.isnan(O)\n",
    "    return (O,N)\n",
    "\n",
    "def sumWithNan(M1,M2):\n",
    "    \"\"\"Add two pairs of (matrix,count)\"\"\"\n",
    "    (X1,N1)=M1\n",
    "    (X2,N2)=M2\n",
    "    N=N1+N2\n",
    "    X=np.nansum(np.dstack((X1,X2)),axis=2)\n",
    "    return (X,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def computeCov(RDDin):\n",
    "    \"\"\"computeCov recieves as input an RDD of np arrays, all of the same length, \n",
    "    and computes the covariance matrix for that set of vectors\"\"\"\n",
    "    RDD=RDDin.map(lambda v:np.array(np.insert(v,0,1),dtype=np.float64)) \n",
    "                     # insert a 1 at the beginning of each vector so that the same \n",
    "                     #calculation also yields the mean vector\n",
    "    OuterRDD=RDD.map(outerProduct)    #<-- do mapping here\n",
    "    (S,N)=OuterRDD.reduce(sumWithNan)  #<-- do reducing here\n",
    "    E=S[0,1:]\n",
    "    NE=np.float64(N[0,1:])\n",
    "    print 'shape of E=',E.shape,'shape of NE=',NE.shape\n",
    "    Mean=E/NE\n",
    "    O=S[1:,1:]\n",
    "    NO=np.float64(N[1:,1:])\n",
    "    (OM,OMN) = outerProduct(Mean)\n",
    "    Cov = O/NO - OM\n",
    "    # Output also the diagnal which is the variance for each day\n",
    "    Var=np.array([Cov[i,i] for i in range(Cov.shape[0])])\n",
    "    return {'E':E,'NE':NE,'O':O,'NO':NO,'Cov':Cov,'Mean':Mean,'Var':Var}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM weather\n",
      "\tWHERE measurement= TMAX \n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "u\"cannot resolve '`TMAX`' given input columns: [measurement, label, elevation, longitude, latitude, station, undefs, year, vector]; line 2 pos 20;\\n'Project [*]\\n+- 'Filter (measurement#3 = 'TMAX)\\n   +- SubqueryAlias weather, `weather`\\n      +- LogicalRDD [elevation#0, latitude#1, longitude#2, measurement#3, station#4, undefs#5L, vector#6, year#7, label#8]\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8084c503eaeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mQuery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SELECT * FROM weather\\n\\tWHERE measurement= %s \"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mQuery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0munpackArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vector'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#get basic statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rajatsharma/Desktop/Courses/CSE255/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \"\"\"\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rajatsharma/Desktop/Courses/CSE255/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \"\"\"\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rajatsharma/Desktop/Courses/CSE255/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rajatsharma/Desktop/Courses/CSE255/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"cannot resolve '`TMAX`' given input columns: [measurement, label, elevation, longitude, latitude, station, undefs, year, vector]; line 2 pos 20;\\n'Project [*]\\n+- 'Filter (measurement#3 = 'TMAX)\\n   +- SubqueryAlias weather, `weather`\\n      +- LogicalRDD [elevation#0, latitude#1, longitude#2, measurement#3, station#4, undefs#5L, vector#6, year#7, label#8]\\n\""
     ]
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "STAT={}  # dictionary storing the statistics for each measurement\n",
    "Clean_Tables={}\n",
    "\n",
    "for meas in measurements:\n",
    "    t=time()\n",
    "    Query=\"SELECT * FROM weather\\n\\tWHERE measurement= '%s' \"%(meas)\n",
    "    print Query\n",
    "    df1 = sqlContext.sql(Query)\n",
    "    data=df1.rdd.map(lambda row: unpackArray(row['vector'],np.float16))\n",
    "    #get basic statistics\n",
    "    STAT[meas]=computeOverAllDist(data)   # Compute the statistics \n",
    "\n",
    "    # compute covariance matrix\n",
    "    OUT=computeCov(data)\n",
    "\n",
    "    #find PCA decomposition\n",
    "    eigval,eigvec=LA.eig(OUT['Cov'])\n",
    "\n",
    "    # collect all of the statistics in STAT[meas]\n",
    "    STAT[meas]['eigval']=eigval\n",
    "    STAT[meas]['eigvec']=eigvec\n",
    "    STAT[meas].update(OUT)\n",
    "\n",
    "    print 'time for',meas,'is',time()-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "filename=data_dir+'/STAT_%s.pickle'%file_index\n",
    "dump((STAT,STAT_Descriptions),open(filename,'wb'))\n",
    "!ls -lrth $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import linalg as LA\n",
    "f = open(\"Tester/hw3.pkl\",'r')\n",
    "thePickle = pickle.load(f)\n",
    "datasets = thePickle[\"pca\"]\n",
    "answers  = thePickle[\"pca_correct\"]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.91839805e-01  -1.56895646e-02  -3.94775805e-01   4.51249590e-01\n",
      "   -5.60284020e-01  -6.30514812e-02  -1.68899783e-01  -2.91810336e-01\n",
      "   -5.64239542e-01   1.05594273e+00]\n",
      " [ -1.56895646e-02   3.10874037e-04   4.89021944e-03   1.34167502e-01\n",
      "   -5.12897658e-03  -3.17390134e-01   6.56611101e-03   6.75567824e-03\n",
      "   -2.85890101e-01   1.43786709e-01]\n",
      " [ -3.94775805e-01   4.89021944e-03   1.29514086e-01  -8.27040078e-02\n",
      "    1.80407303e-01  -5.13288358e-02   5.66731359e-02   9.66203363e-02\n",
      "    7.70841528e-02  -2.24940912e-01]\n",
      " [  4.51249590e-01   1.34167502e-01  -8.27040078e-02   4.46724353e-02\n",
      "   -1.25950151e-01  -9.94657259e-02  -3.22060779e-02  -5.89021614e-02\n",
      "   -3.66669765e-01   2.67964568e-02]\n",
      " [ -5.60284020e-01  -5.12897658e-03   1.80407303e-01  -1.25950151e-01\n",
      "    2.51943088e-01  -5.67817967e-02   7.87044799e-02   1.34420240e-01\n",
      "    1.20014752e-01  -3.12629585e-01]\n",
      " [ -6.30514812e-02  -3.17390134e-01  -5.13288358e-02  -9.94657259e-02\n",
      "   -5.67817967e-02   2.21466114e-01  -2.79159092e-02  -4.21223575e-02\n",
      "    3.52111625e-01   1.97765120e-01]\n",
      " [ -1.68899783e-01   6.56611101e-03   5.66731359e-02  -3.22060779e-02\n",
      "    7.87044799e-02  -2.79159092e-02   2.48876529e-02   4.23415033e-02\n",
      "    2.90452995e-02  -9.86908606e-02]\n",
      " [ -2.91810336e-01   6.75567824e-03   9.66203363e-02  -5.89021614e-02\n",
      "    1.34420240e-01  -4.21223575e-02   4.23415033e-02   7.21244828e-02\n",
      "    5.42170453e-02  -1.67993779e-01]\n",
      " [ -5.64239542e-01  -2.85890101e-01   7.70841528e-02  -3.66669765e-01\n",
      "    1.20014752e-01   3.52111625e-01   2.90452995e-02   5.42170453e-02\n",
      "    1.92497793e-01  -1.06729608e-01]\n",
      " [  1.05594273e+00   1.43786709e-01  -2.24940912e-01   2.67964568e-02\n",
      "   -3.12629585e-01   1.97765120e-01  -9.86908606e-02  -1.67993779e-01\n",
      "   -1.06729608e-01   5.91757917e-02]]\n"
     ]
    }
   ],
   "source": [
    "corAns = answers[0]\n",
    "print corAns['Cov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.60479683 -0.66510572  0.22128462  0.57481219  0.11641649 -0.22160924\n",
      "  0.16793543  0.21500296 -0.10286185 -0.7780524 ]\n"
     ]
    }
   ],
   "source": [
    "print corAns['Mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.31523801  0.77312853 -1.05721608  0.103605   -1.26138469  0.07097709\n",
      "  -0.54093319 -0.84368688 -0.50202902  1.52650636]\n",
      " [ 0.77312853  0.88535298 -0.2845749  -0.24814338 -0.1651165  -0.16999656\n",
      "  -0.2102574  -0.27248803 -0.2174761   0.66127381]\n",
      " [-1.05721608 -0.2845749   0.53544291  0.08898618  0.61850545 -0.20073511\n",
      "   0.28150399  0.43259155  0.10864482 -0.79422389]\n",
      " [ 0.103605   -0.24814338  0.08898618  0.75016298 -0.11806507 -0.45369884\n",
      "   0.12865051  0.12936832 -0.42579601 -0.42043755]\n",
      " [-1.26138469 -0.1651165   0.61850545 -0.11806507  0.79648766 -0.16516153\n",
      "   0.2947648   0.47835039  0.21607987 -0.80641543]\n",
      " [ 0.07097709 -0.16999656 -0.20073511 -0.45369884 -0.16516153  0.54115354\n",
      "  -0.1302639  -0.179538    0.37490676  0.37018872]\n",
      " [-0.54093319 -0.2102574   0.28150399  0.12865051  0.2947648  -0.1302639\n",
      "   0.15926988  0.23534435  0.0235423  -0.45870685]\n",
      " [-0.84368688 -0.27248803  0.43259155  0.12936832  0.47835039 -0.179538\n",
      "   0.23534435  0.35505226  0.06420289 -0.67055469]\n",
      " [-0.50202902 -0.2174761   0.10864482 -0.42579601  0.21607987  0.37490676\n",
      "   0.0235423   0.06420289  0.40615671 -0.0533954 ]\n",
      " [ 1.52650636  0.66127381 -0.79422389 -0.42043755 -0.80641543  0.37018872\n",
      "  -0.45870685 -0.67055469 -0.0533954   1.32908267]]\n"
     ]
    }
   ],
   "source": [
    "print corAns['O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  2.  2.  1.  2.  1.  2.  2.  1.  1.]\n",
      " [ 2.  2.  2.  1.  2.  1.  2.  2.  1.  1.]\n",
      " [ 2.  2.  3.  2.  3.  2.  3.  3.  2.  2.]\n",
      " [ 1.  1.  2.  2.  2.  2.  2.  2.  1.  1.]\n",
      " [ 2.  2.  3.  2.  3.  2.  3.  3.  2.  2.]\n",
      " [ 1.  1.  2.  2.  2.  2.  2.  2.  1.  1.]\n",
      " [ 2.  2.  3.  2.  3.  2.  3.  3.  2.  2.]\n",
      " [ 2.  2.  3.  2.  3.  2.  3.  3.  2.  2.]\n",
      " [ 1.  1.  2.  1.  2.  1.  2.  2.  2.  2.]\n",
      " [ 1.  1.  2.  1.  2.  1.  2.  2.  2.  2.]]\n"
     ]
    }
   ],
   "source": [
    "print corAns['NO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XX = corAns['O'] - corAns['Mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.92003484  1.43823425 -1.27850071 -0.47120719 -1.37780118  0.29258633\n",
      "  -0.70886862 -1.05868984 -0.39916718  2.30455876]\n",
      " [ 1.37792536  1.5504587  -0.50585952 -0.82295557 -0.28153299  0.05161268\n",
      "  -0.37819283 -0.48749099 -0.11461425  1.43932622]\n",
      " [-0.45241926  0.38053082  0.31415829 -0.48582601  0.50208895  0.02087414\n",
      "   0.11356856  0.2175886   0.21150666 -0.01617148]\n",
      " [ 0.70840183  0.41696234 -0.13229844  0.17535079 -0.23448156 -0.2320896\n",
      "  -0.03928492 -0.08563464 -0.32293416  0.35761485]\n",
      " [-0.65658786  0.49998922  0.39722082 -0.69287726  0.68007117  0.05644771\n",
      "   0.12682937  0.26334743  0.31894172 -0.02836303]\n",
      " [ 0.67577391  0.49510916 -0.42201973 -1.02851103 -0.28157802  0.76276278\n",
      "  -0.29819933 -0.39454095  0.47776861  1.14824113]\n",
      " [ 0.06386363  0.45484832  0.06021937 -0.44616169  0.17834831  0.09134534\n",
      "  -0.00866555  0.02034139  0.12640415  0.31934556]\n",
      " [-0.23889006  0.39261769  0.21130693 -0.44544388  0.3619339   0.04207124\n",
      "   0.06740892  0.14004931  0.16706473  0.10749771]\n",
      " [ 0.1027678   0.44762962 -0.1126398  -1.0006082   0.09966338  0.596516\n",
      "  -0.14439312 -0.15080007  0.50901855  0.724657  ]\n",
      " [ 2.13130318  1.32637953 -1.01550851 -0.99524974 -0.92283192  0.59179797\n",
      "  -0.62664227 -0.88555765  0.04946644  2.10713508]]\n"
     ]
    }
   ],
   "source": [
    "print XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.15761901  0.38656426 -0.52860804  0.103605   -0.63069234  0.07097709\n",
      "  -0.2704666  -0.42184344 -0.50202902  1.52650636]\n",
      " [ 0.38656426  0.44267649 -0.14228745 -0.24814338 -0.08255825 -0.16999656\n",
      "  -0.1051287  -0.13624402 -0.2174761   0.66127381]\n",
      " [-0.52860804 -0.14228745  0.17848097  0.04449309  0.20616848 -0.10036755\n",
      "   0.09383466  0.14419718  0.05432241 -0.39711194]\n",
      " [ 0.103605   -0.24814338  0.04449309  0.37508149 -0.05903253 -0.22684942\n",
      "   0.06432525  0.06468416 -0.42579601 -0.42043755]\n",
      " [-0.63069234 -0.08255825  0.20616848 -0.05903253  0.26549589 -0.08258077\n",
      "   0.09825493  0.15945013  0.10803994 -0.40320772]\n",
      " [ 0.07097709 -0.16999656 -0.10036755 -0.22684942 -0.08258077  0.27057677\n",
      "  -0.06513195 -0.089769    0.37490676  0.37018872]\n",
      " [-0.2704666  -0.1051287   0.09383466  0.06432525  0.09825493 -0.06513195\n",
      "   0.05308996  0.07844812  0.01177115 -0.22935342]\n",
      " [-0.42184344 -0.13624402  0.14419718  0.06468416  0.15945013 -0.089769\n",
      "   0.07844812  0.11835075  0.03210144 -0.33527735]\n",
      " [-0.50202902 -0.2174761   0.05432241 -0.42579601  0.10803994  0.37490676\n",
      "   0.01177115  0.03210144  0.20307835 -0.0266977 ]\n",
      " [ 1.52650636  0.66127381 -0.39711194 -0.42043755 -0.40320772  0.37018872\n",
      "  -0.22935342 -0.33527735 -0.0266977   0.66454134]]\n"
     ]
    }
   ],
   "source": [
    "YY = np.divide(corAns['O'], corAns['NO'])\n",
    "print YY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.91839805e-01  -1.56895646e-02  -3.94775805e-01   4.51249590e-01\n",
      "   -5.60284020e-01  -6.30514812e-02  -1.68899783e-01  -2.91810336e-01\n",
      "   -5.64239542e-01   1.05594273e+00]\n",
      " [ -1.56895646e-02   3.10874037e-04   4.89021944e-03   1.34167502e-01\n",
      "   -5.12897658e-03  -3.17390134e-01   6.56611101e-03   6.75567824e-03\n",
      "   -2.85890101e-01   1.43786709e-01]\n",
      " [ -3.94775805e-01   4.89021944e-03   1.29514086e-01  -8.27040078e-02\n",
      "    1.80407303e-01  -5.13288358e-02   5.66731359e-02   9.66203363e-02\n",
      "    7.70841528e-02  -2.24940912e-01]\n",
      " [  4.51249590e-01   1.34167502e-01  -8.27040078e-02   4.46724353e-02\n",
      "   -1.25950151e-01  -9.94657259e-02  -3.22060779e-02  -5.89021614e-02\n",
      "   -3.66669765e-01   2.67964568e-02]\n",
      " [ -5.60284020e-01  -5.12897658e-03   1.80407303e-01  -1.25950151e-01\n",
      "    2.51943088e-01  -5.67817967e-02   7.87044799e-02   1.34420240e-01\n",
      "    1.20014752e-01  -3.12629585e-01]\n",
      " [ -6.30514812e-02  -3.17390134e-01  -5.13288358e-02  -9.94657259e-02\n",
      "   -5.67817967e-02   2.21466114e-01  -2.79159092e-02  -4.21223575e-02\n",
      "    3.52111625e-01   1.97765120e-01]\n",
      " [ -1.68899783e-01   6.56611101e-03   5.66731359e-02  -3.22060779e-02\n",
      "    7.87044799e-02  -2.79159092e-02   2.48876529e-02   4.23415033e-02\n",
      "    2.90452995e-02  -9.86908606e-02]\n",
      " [ -2.91810336e-01   6.75567824e-03   9.66203363e-02  -5.89021614e-02\n",
      "    1.34420240e-01  -4.21223575e-02   4.23415033e-02   7.21244828e-02\n",
      "    5.42170453e-02  -1.67993779e-01]\n",
      " [ -5.64239542e-01  -2.85890101e-01   7.70841528e-02  -3.66669765e-01\n",
      "    1.20014752e-01   3.52111625e-01   2.90452995e-02   5.42170453e-02\n",
      "    1.92497793e-01  -1.06729608e-01]\n",
      " [  1.05594273e+00   1.43786709e-01  -2.24940912e-01   2.67964568e-02\n",
      "   -3.12629585e-01   1.97765120e-01  -9.86908606e-02  -1.67993779e-01\n",
      "   -1.06729608e-01   5.91757917e-02]]\n"
     ]
    }
   ],
   "source": [
    "mm = corAns['Mean']\n",
    "xxx = np.dot(mm.T,mm)\n",
    "xxx = np.outer(mm,mm)\n",
    "lll = YY - xxx\n",
    "print lll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.91839805e-01  -1.56895646e-02  -3.94775805e-01   4.51249590e-01\n",
      "   -5.60284020e-01  -6.30514812e-02  -1.68899783e-01  -2.91810336e-01\n",
      "   -5.64239542e-01   1.05594273e+00]\n",
      " [ -1.56895646e-02   3.10874037e-04   4.89021944e-03   1.34167502e-01\n",
      "   -5.12897658e-03  -3.17390134e-01   6.56611101e-03   6.75567824e-03\n",
      "   -2.85890101e-01   1.43786709e-01]\n",
      " [ -3.94775805e-01   4.89021944e-03   1.29514086e-01  -8.27040078e-02\n",
      "    1.80407303e-01  -5.13288358e-02   5.66731359e-02   9.66203363e-02\n",
      "    7.70841528e-02  -2.24940912e-01]\n",
      " [  4.51249590e-01   1.34167502e-01  -8.27040078e-02   4.46724353e-02\n",
      "   -1.25950151e-01  -9.94657259e-02  -3.22060779e-02  -5.89021614e-02\n",
      "   -3.66669765e-01   2.67964568e-02]\n",
      " [ -5.60284020e-01  -5.12897658e-03   1.80407303e-01  -1.25950151e-01\n",
      "    2.51943088e-01  -5.67817967e-02   7.87044799e-02   1.34420240e-01\n",
      "    1.20014752e-01  -3.12629585e-01]\n",
      " [ -6.30514812e-02  -3.17390134e-01  -5.13288358e-02  -9.94657259e-02\n",
      "   -5.67817967e-02   2.21466114e-01  -2.79159092e-02  -4.21223575e-02\n",
      "    3.52111625e-01   1.97765120e-01]\n",
      " [ -1.68899783e-01   6.56611101e-03   5.66731359e-02  -3.22060779e-02\n",
      "    7.87044799e-02  -2.79159092e-02   2.48876529e-02   4.23415033e-02\n",
      "    2.90452995e-02  -9.86908606e-02]\n",
      " [ -2.91810336e-01   6.75567824e-03   9.66203363e-02  -5.89021614e-02\n",
      "    1.34420240e-01  -4.21223575e-02   4.23415033e-02   7.21244828e-02\n",
      "    5.42170453e-02  -1.67993779e-01]\n",
      " [ -5.64239542e-01  -2.85890101e-01   7.70841528e-02  -3.66669765e-01\n",
      "    1.20014752e-01   3.52111625e-01   2.90452995e-02   5.42170453e-02\n",
      "    1.92497793e-01  -1.06729608e-01]\n",
      " [  1.05594273e+00   1.43786709e-01  -2.24940912e-01   2.67964568e-02\n",
      "   -3.12629585e-01   1.97765120e-01  -9.86908606e-02  -1.67993779e-01\n",
      "   -1.06729608e-01   5.91757917e-02]]\n"
     ]
    }
   ],
   "source": [
    "print corAns['Cov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": "PCA_using_numpy for HW3",
  "notebookId": 85286,
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "245px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
