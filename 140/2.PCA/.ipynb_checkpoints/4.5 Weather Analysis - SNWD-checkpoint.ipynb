{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#setup\n",
    "data_dir ='../../Data/Weather'\n",
    "file_index='SSSBSSSS'#BBBSBBBB'\n",
    "m='SNWD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "variables": {
     "m": "SNWD"
    }
   },
   "source": [
    "## Reconstruction using top eigen-vectors\n",
    "For measurement = {{m}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Enable automiatic reload of libraries\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2 # means that all modules are reloaded before every command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "    pandas as    pd \tversion=0.19.2 \trequired version>=0.19.2\n",
      "     numpy as    np \tversion=1.12.1 \trequired version>=1.12.0\n",
      "   sklearn as    sk \tversion=0.18.1 \trequired version>=0.18.1\n",
      "    urllib as urllib \tversion=1.17 \trequired version>=1.17\n",
      "   pyspark as pyspark \tversion=2.1.0+hadoop2.7 \trequired version>=2.1.0\n",
      "ipywidgets as ipywidgets \tversion=6.0.0 \trequired version>=6.0.0\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import sys\n",
    "sys.path.append('./lib')\n",
    "\n",
    "from numpy_pack import packArray,unpackArray\n",
    "\n",
    "from Eigen_decomp import Eigen_decomp\n",
    "from YearPlotter import YearPlotter\n",
    "from recon_plot import recon_plot\n",
    "\n",
    "from import_modules import import_modules,modules\n",
    "import_modules(modules)\n",
    "\n",
    "from ipywidgets import interactive,widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-193c0320f482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# sc.stop()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"local[3]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpyFiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lib/numpy_pack.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lib/spark_PCA.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lib/computeStats.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lib/recon_plot.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lib/Eigen_decomp.py'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dhruvds/dhruv/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 118\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dhruvds/dhruv/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dhruvds/dhruv/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dhruvds/dhruv/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1397\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1400\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1401\u001b[0m             answer, self._gateway_client, None, self._fqn)\n",
      "\u001b[0;32m/home/dhruvds/dhruv/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dhruvds/dhruv/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "# sc.stop()\n",
    "\n",
    "sc = SparkContext(master=\"local[3]\",pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStats.py','lib/recon_plot.py','lib/Eigen_decomp.py'])\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read Statistics File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "#read statistics\n",
    "filename=data_dir+'/STAT_%s.pickle'%file_index\n",
    "STAT,STAT_Descriptions = load(open(filename,'rb'))\n",
    "measurements=STAT.keys()\n",
    "print 'keys from STAT=',measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read data file into a spark DataFrame\n",
    "We focus on the snow-depth records, because the eigen-vectors for them make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "filename=data_dir+'/US_Weather_%s.parquet'%file_index\n",
    "df_in=sqlContext.read.parquet(filename)\n",
    "#filter in \n",
    "df=df_in.filter(df_in.measurement==m)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plot Reconstructions\n",
    "\n",
    "Construct approximations of a time series using the mean and the $k$ top eigen-vectors\n",
    "First, we plot the mean and the top $k$ eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "YP=YearPlotter()\n",
    "def plot_pair(pair,func,metric=None):\n",
    "    j=0\n",
    "    fig,X=subplots(len(pair)/2,2,figsize=(20,6*len(pair)/2))\n",
    "#     print X\n",
    "    axes=X.reshape(len(pair))\n",
    "    for m in pair:\n",
    "        axis = axes[j]\n",
    "        j+=1\n",
    "        if not metric:\n",
    "            func(m,fig,axis)\n",
    "        else:\n",
    "            func(m,fig,axis,metric[m],m)\n",
    "def plot_single(pair,func,metric=None):\n",
    "    j=0\n",
    "    fig,axis=subplots(1,1,figsize=(10,6))\n",
    "#     print X,fig\n",
    "    if not metric:\n",
    "            func(m,fig,axis)\n",
    "    else:\n",
    "        func(m,fig,axis,metric[m],m)\n",
    "def plot_valid(m,fig,axis,metric='mean'):\n",
    "    valid_m=STAT[m][metric]\n",
    "    YearPlotter().plot(valid_m,fig,axis,title='mean-counts '+m)\n",
    "def plot_mean_std(m,fig,axis):\n",
    "    mean=STAT[m]['Mean']\n",
    "    std=np.sqrt(STAT[m]['Var'])\n",
    "    graphs=np.vstack([mean,mean-std,mean+std]).transpose()\n",
    "    YP.plot(graphs,fig,axis,title='Mean+-std   '+m,labels=['mean','mean-std','mean+std'])\n",
    "def plot_mean(m,fig,axis,metric,name):\n",
    "    year = metrics[name]['year'].tolist()\n",
    "    metric = metrics[name]['avg(mean)'].tolist()\n",
    "    year = [year[i] for i in range(0,len(year)-1,2)]\n",
    "    metric = [(metric[i]+metric[i+1])*1.0/2 for i in range(0,len(metric)-1,2)]\n",
    "    axis.plot(year,metric)\n",
    "    axis.set_title(name)\n",
    "def plot_eigen(m,fig,axis):\n",
    "    EV=STAT[m]['eigvec']\n",
    "    YP.plot(EV[:,:3],fig,axis,title='Top Eigenvectors '+m)\n",
    "def pltVarExplained(j):\n",
    "    subplot(1,3,j)\n",
    "    EV=STAT[m]['eigval']\n",
    "    k=5\n",
    "    plot(([0,]+list(cumsum(EV[:k])))/sum(EV))\n",
    "    title('Percentage of Variance Explained for '+ m)\n",
    "    ylabel('Percentage of Variance')\n",
    "    xlabel('# Eigenvector')\n",
    "    grid()\n",
    "#     plt.show()\n",
    "#     YP.plot(metric,fig,axis,title=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "\n",
    "k=3\n",
    "EigVec=np.matrix(STAT[m]['eigvec'][:,:k])\n",
    "Mean=STAT[m]['Mean']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(2,1, sharex='col', sharey='row',figsize=(10,6))\n",
    "plot_single([m],plot_mean_std)\n",
    "YearPlotter().plot(Mean,fig,axes[0],label='Mean',title=m+' Mean')\n",
    "\n",
    "YearPlotter().plot(EigVec,fig,axes[1],title=m+' Eigs',labels=['eig'+str(i+1) for i in range(k)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "v=[np.array(EigVec[:,i]).flatten() for i in range(np.shape(EigVec)[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### plot the percent of residual variance on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  x=0 in the graphs below correspond to the fraction of the variance explained by the mean alone\n",
    "#  x=1,2,3,... are the residuals for eig1, eig1+eig2, eig1+eig2+eig3 ...\n",
    "fig,ax=plt.subplots(1,1);\n",
    "eigvals=STAT[m]['eigval']; eigvals/=sum(eigvals); cumvar=np.cumsum(eigvals); cumvar=100*np.insert(cumvar,0,0)\n",
    "ax.plot(cumvar[:10]); \n",
    "ax.grid(); \n",
    "ax.set_ylabel('Percent of variance explained')\n",
    "ax.set_xlabel('number of eigenvectors')\n",
    "ax.set_title('Percent of variance explained');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Process whole dataframe to find best and worse residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Add to each row in the dataframe a residual values \n",
    "Residuals are after subtracting in sequence: the mean, the projection on the first eigen-vector the projection on the second eigen-vector etc.\n",
    "\n",
    "`decompose(row)` axtracts the series from the row, computes the residuals and constructs a new row that is reassembled into a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def decompose(row):\n",
    "    \"\"\"compute residual and coefficients for decomposition           \n",
    "\n",
    "    :param row: SparkSQL Row that contains the measurements for a particular station, year and measurement. \n",
    "    :returns: the input row with additional information from the eigen-decomposition.\n",
    "    :rtype: SparkSQL Row \n",
    "\n",
    "    Note that Decompose is designed to run inside a spark \"map()\" command.\n",
    "    Mean and v are sent to the workers as local variables of \"Decompose\"\n",
    "\n",
    "    \"\"\"\n",
    "    Series=np.array(unpackArray(row.vector,np.float16),dtype=np.float64)\n",
    "    recon=Eigen_decomp(None,Series,Mean,v);\n",
    "    total_var,residuals,reductions,coeff=recon.compute_var_explained()\n",
    "    #print coeff\n",
    "    residuals=[float(r) for r in residuals[1]]\n",
    "    coeff=[float(r) for r in coeff[1]]\n",
    "    D=row.asDict()\n",
    "    D['total_var']=float(total_var[1])\n",
    "    D['res_mean']=residuals[0]\n",
    "    for i in range(1,len(residuals)):\n",
    "        D['res_'+str(i)]=residuals[i]\n",
    "        D['coeff_'+str(i)]=coeff[i-1]\n",
    "    return Row(**D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "rdd2=df.rdd.map(decompose)\n",
    "df2=sqlContext.createDataFrame(rdd2)\n",
    "row,=df2.take(1)\n",
    "\n",
    "#filter out vectors for which the mean is a worse approximation than zero.\n",
    "print 'before filter',df2.count()\n",
    "df3=df2.filter(df2.res_mean<1)\n",
    "print 'after filter',df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_decomp(row,Mean,v,fig=None,ax=None,Title=None,interactive=False):\n",
    "    \"\"\"Plot a single reconstruction with an informative title\n",
    "\n",
    "    :param row: SparkSQL Row that contains the measurements for a particular station, year and measurement. \n",
    "    :param Mean: The mean vector of all measurements of a given type\n",
    "    :param v: eigen-vectors for the distribution of measurements.\n",
    "    :param fig: a matplotlib figure in which to place the plot\n",
    "    :param ax: a matplotlib axis in which to place the plot\n",
    "    :param Title: A plot title over-ride.\n",
    "    :param interactive: A flag that indicates whether or not this is an interactive plot (widget-driven)\n",
    "    :returns: a plotter returned by recon_plot initialization\n",
    "    :rtype: recon_plot\n",
    "\n",
    "    \"\"\"\n",
    "    target=np.array(unpackArray(row.vector,np.float16),dtype=np.float64)\n",
    "    if Title is None:\n",
    "        Title='%s / %d    %s'%(row['station'],row['year'],row['measurement'])\n",
    "    eigen_decomp=Eigen_decomp(range(1,366),target,Mean,v)\n",
    "    plotter=recon_plot(eigen_decomp,year_axis=True,fig=fig,ax=ax,interactive=interactive,Title=Title)\n",
    "    return plotter\n",
    "\n",
    "def plot_recon_grid(rows,column_n=3, row_n=2, figsize=(20,8) , resNum=1):\n",
    "    \"\"\"plot a grid of reconstruction plots\n",
    "\n",
    "    :param rows: Data rows (as extracted from the measurements data-frame\n",
    "    :param column_n: number of columns\n",
    "    :param row_n:  number of rows\n",
    "    :param figsize: Size of figure\n",
    "    :returns: None\n",
    "    :rtype: \n",
    "\n",
    "    \"\"\"\n",
    "    fig,axes=plt.subplots(row_n,column_n, sharex='col', sharey='row',figsize=figsize);\n",
    "#     print axes\n",
    "    k=0\n",
    "    for i in range(row_n):\n",
    "        for j in range(column_n):\n",
    "            row=rows[k]\n",
    "            k+=1\n",
    "            _title = 'coeff_%1.0f=%3.1f,res_3=%3.2f'%(resNum,row['coeff_'+str(resNum)],row['res_'+str(3)])\n",
    "            if row_n>1:\n",
    "                plot_decomp(row,Mean,v,fig=fig,ax=axes[i,j],Title=_title,interactive=False)\n",
    "            else:\n",
    "                plot_decomp(row,Mean,v,fig=fig,ax=axes[j],Title=_title,interactive=False)\n",
    "                \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# A function for plotting the CDF of a given feature\n",
    "def plot_CDF(feat,fig=None,axis=None):\n",
    "    rows=df3.select(feat).sort(feat).collect()\n",
    "    vals=[r[feat] for r in rows]\n",
    "    P=np.arange(0,1.00001,1./(len(vals)))\n",
    "    vals=[vals[0]]+vals\n",
    "    if axis:\n",
    "        axis.plot(vals,P)\n",
    "        axis.set_title('cumulative distribution of '+feat)\n",
    "        axis.set_ylabel('number of instances')\n",
    "        axis.set_xlabel(feat)\n",
    "        axis.grid()\n",
    "        \n",
    "    else:\n",
    "        plot(vals,P)\n",
    "        title('cumulative distribution of '+feat)\n",
    "        ylabel('number of instances')\n",
    "        xlabel(feat)\n",
    "        grid()\n",
    "\n",
    "# A function for plotting the CDF of a given feature\n",
    "def plot_CDF_filter(feat,fig=None,axis=None):\n",
    "    rows=df3.filter(df3.res_3<0.4).select(feat).sort(feat).collect()\n",
    "    vals=[r[feat] for r in rows]\n",
    "    P=np.arange(0,1.00001,1./(len(vals)))\n",
    "    vals=[vals[0]]+vals\n",
    "    if axis:\n",
    "        axis.plot(vals,P)\n",
    "        axis.set_title('cumulative distribution of '+feat + \" with res_3 < 0.4\")\n",
    "        axis.set_ylabel('number of instances')\n",
    "        axis.set_xlabel(feat)\n",
    "        axis.grid()\n",
    "    else:\n",
    "        plot(vals,P)\n",
    "        title('cumulative distribution of '+feat)\n",
    "        ylabel('number of instances')\n",
    "        xlabel(feat)\n",
    "        grid()\n",
    "\n",
    "def plot_cdf_grid(num,column_n=3, row_n=1, figsize=(20,8)):\n",
    "    \n",
    "    fig,axes=plt.subplots(row_n,column_n, sharex='col', sharey='row',figsize=figsize);\n",
    "#     print axes\n",
    "    k=0\n",
    "#         _title = 'coeff_%1.0f=%3.1f,res_3=%3.2f'%(resNum,row['coeff_'+str(resNum)],row['res_'+str(3)])\n",
    "#         else:\n",
    "    plot_CDF('res_'+str(num),fig,axes[0])\n",
    "    plot_CDF('coeff_'+str(num),fig,axes[1])\n",
    "    plot_CDF_filter('coeff_'+str(num),fig,axes[2])\n",
    "                \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CDF('res_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Sort entries by increasing values of ers_3\n",
    "df3=df3.sort(df3.res_3,ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coeff 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df4=df3.filter(df3.res_3<0.4).sort(df3.coeff_1)\n",
    "rows=df4.take(6)\n",
    "df4.select('coeff_1','res_1').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "plot_recon_grid(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df5=df3.filter(df3.res_3<0.4).sort(df3.coeff_1,ascending=False)\n",
    "rows=df5.take(6)\n",
    "df5.select('coeff_1','res_1').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "plot_recon_grid(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "row=rows[4]\n",
    "target=np.array(unpackArray(row.vector,np.float16),dtype=np.float64)\n",
    "eigen_decomp=Eigen_decomp(None,target,Mean,v)\n",
    "print eigen_decomp.coeff\n",
    "total_var,residuals,reductions,coeff=eigen_decomp.compute_var_explained()\n",
    "res=residuals[1]\n",
    "print 'residual normalized norm  after mean:',res[0]\n",
    "print 'residual normalized norm  after mean + top eigs:',res[1:]\n",
    "\n",
    "# plotter=recon_plot(eigen_decomp,year_axis=True,interactive=True)\n",
    "# display(plotter.get_Interactive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf_grid(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coeff 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df4=df3.filter(df3.res_3<0.4).sort(df3.coeff_2)\n",
    "rows=df4.take(6)\n",
    "df4.select('coeff_2','res_2').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "plot_recon_grid(rows,resNum=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df4=df3.filter(df3.res_3<0.4).sort(df3.coeff_2,ascending=False)\n",
    "rows=df4.take(6)\n",
    "df4.select('coeff_2','res_2').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_recon_grid(rows,resNum=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf_grid(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coeff 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df4=df3.filter(df3.res_3<0.4).sort(df3.coeff_3)\n",
    "rows=df4.take(6)\n",
    "df4.select('coeff_3','res_3').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "plot_recon_grid(rows,resNum=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df4=df3.filter(df3.res_3<0.4).sort(df3.coeff_3)\n",
    "rows=df4.take(6)\n",
    "df4.select('coeff_3','res_3').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_recon_grid(rows,resNum=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf_grid(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# filename=data_dir+'/decon_'+file_index+'_'+m+'.parquet'\n",
    "# !rm -rf $filename\n",
    "# df3.write.parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#extract longitude and latitude for each station\n",
    "def RMS(Mat):\n",
    "    return np.sqrt(np.nanmean(Mat**2))\n",
    "\n",
    "# sqlContext.registerDataFrameAsTable(df,'weather')\n",
    "\n",
    "#Features=', '.join(['coeff_1', 'coeff_2', 'coeff_3', 'elevation', 'latitude', 'longitude',\\\n",
    "#          'res_1', 'res_2', 'res_3', 'res_mean', 'year'])\n",
    "for feature in ['coeff_1','coeff_2','coeff_3']:\n",
    "    Features=['station', 'year', feature]\n",
    "    _df = df3.select(Features)\n",
    "#     Query=\"SELECT %s FROM weather\"%Features\n",
    "#     print(Query)\n",
    "#     pdf = sqlContext.sql(Query).toPandas()\n",
    "    pdf = _df.toPandas()\n",
    "    # pdf.head()\n",
    "    year_station_table=pdf.pivot(index='year', columns='station', values=feature)\n",
    "    # year_station_table.head(10)\n",
    "    mean_by_year=np.nanmean(year_station_table,axis=1)\n",
    "    mean_by_station=np.nanmean(year_station_table,axis=0)\n",
    "    tbl_minus_year = (year_station_table.transpose()-mean_by_year).transpose()\n",
    "    rms_yr = RMS(tbl_minus_year)\n",
    "    tbl_minus_station = year_station_table-mean_by_station\n",
    "    rms_st = RMS(tbl_minus_station)\n",
    "    rms = RMS(year_station_table)\n",
    "\n",
    "    print feature\n",
    "    print 'total RMS                   = ', rms\n",
    "    print 'RMS removing mean-by-station= ', rms_st,  'variance explained', (rms**2-rms_st**2)/rms**2\n",
    "    print 'RMS removing mean-by-year   = ', rms_yr,  'variance explained', (rms**2-rms_yr**2)/rms**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=df.rdd.map(lambda row:(row.station,row.year,unpackArray(row['vector'],np.float16))).collect()\n",
    "days=set([r[1] for r in rows])\n",
    "miny=min(days)\n",
    "maxy=max(days)\n",
    "record_len=int((maxy-miny+1)*365)\n",
    "record_len\n",
    "## combine the measurements for each station into a single long array with an entry for each day of each day\n",
    "All={}  # a dictionary with a numpy array for each day of each day\n",
    "i=0\n",
    "for station,day,vector in rows:\n",
    "    i+=1; \n",
    "    # if i%1000==0: print i,len(All)\n",
    "    if not station in All:\n",
    "        a=np.zeros(record_len)\n",
    "        a.fill(np.nan)\n",
    "        All[station]=a\n",
    "    loc = int((day-miny)*365)\n",
    "    All[station][loc:loc+365]=vector\n",
    "from datetime import date\n",
    "d=datetime.date(int(miny), month=1, day=1)\n",
    "start=d.toordinal()\n",
    "dates=[date.fromordinal(i) for i in range(start,start+record_len)]\n",
    "Stations=sorted(All.keys())\n",
    "A=[]\n",
    "for station in Stations:\n",
    "    A.append(All[station])\n",
    "\n",
    "day_station_table=np.hstack([A])\n",
    "print shape(day_station_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_by_day=np.nanmean(day_station_table,axis=0)\n",
    "mean_by_station=np.nanmean(day_station_table,axis=1)\n",
    "tbl_minus_day = day_station_table-mean_by_day\n",
    "tbl_minus_station = (day_station_table.transpose()-mean_by_station).transpose()\n",
    "\n",
    "print 'total RMS                   = ',RMS(day_station_table)\n",
    "print 'RMS removing mean-by-station= ',RMS(tbl_minus_station)\n",
    "print 'RMS removing mean-by-day   = ',RMS(tbl_minus_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT=day_station_table\n",
    "F=RT.flatten()\n",
    "NN=F[~np.isnan(F)]\n",
    "\n",
    "NN.sort()\n",
    "P=np.arange(0.,1.,1./len(NN))\n",
    "NN = NN[:min(len(P),len(NN))]\n",
    "P = P[:len(NN)]\n",
    "print NN.shape,P.shape\n",
    "plot(NN,P)\n",
    "grid()\n",
    "title('CDF of daily %s'%(m))\n",
    "xlabel('daily value')\n",
    "ylabel('cumulative probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "print day_station_table.shape\n",
    "P_norm=pd.DataFrame(day_station_table.T).corr()\n",
    "print P_norm.shape\n",
    "# print P_norm\n",
    "print type(P_norm),P_norm.shape\n",
    "P_norm = P_norm.as_matrix()\n",
    "for i in range(len(P_norm)):\n",
    "    P_norm[i,i]=np.nan\n",
    "print P_norm.shape\n",
    "A=P_norm.flatten();\n",
    "B=A[~isnan(A)]\n",
    "print A.shape,B.shape\n",
    "hist(B,bins=100);\n",
    "xlabel('significance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "P_norm0 = np.nan_to_num(P_norm)\n",
    "n_comp=10\n",
    "pca = PCA(n_components=n_comp, svd_solver='full')\n",
    "pca.fit(P_norm0)     \n",
    "#print(pca.explained_variance_)\n",
    "Var_explained=pca.explained_variance_ratio_\n",
    "plot(insert(cumsum(Var_explained),0,0))\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will look only at the top 4 eigenvectors.\n",
    "n_comp=4\n",
    "pca = PCA(n_components=n_comp, svd_solver='full')\n",
    "pca.fit(P_norm0)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(1,4,figsize=(20,5),sharey='row');\n",
    "L=list(pca.components_.transpose())\n",
    "for i in range(4):\n",
    "    X=sorted(L,key=lambda x:x[i]) \n",
    "    axes[i].plot(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def re_order_matrix(M,order):\n",
    "    M_reord=M[order,:]\n",
    "    M_reord=M_reord[:,order]\n",
    "    return M_reord\n",
    "def showmat(mat):\n",
    "    fig,axes=plt.subplots(1,1,figsize=(6,6))\n",
    "    axes.imshow(mat, cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,axes=plt.subplots(2,2,figsize=(15,15),sharex='col',sharey='row');\n",
    "\n",
    "i=0\n",
    "showmat(P_norm0)\n",
    "for r in range(2):\n",
    "    for c in range(2):\n",
    "        order=np.argsort(pca.components_[i,:])\n",
    "        P_norm_reord=re_order_matrix(P_norm0,order)\n",
    "#         axes[r,c].matshow(P_norm_reord)\n",
    "        showmat(P_norm_reord)\n",
    "        i+=1\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlated=[]\n",
    "for i in range(len(P_norm)):\n",
    "    for j in range(len(P_norm)):\n",
    "        if P_norm[i][j]>0.8:\n",
    "            correlated+=[(i,j)]\n",
    "# correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(data_dir+'/stations.pkl','rb') as file:\n",
    "    all_stations=load(file)\n",
    "all_stations=all_stations[all_stations.index.isin(Stations) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_points = all_stations\n",
    "correlated_points = []\n",
    "for cor in correlated:\n",
    "    correlated_points +=[ (all_stations[all_stations.index==Stations[cor[0]]],all_stations[all_stations.index==Stations[cor[1]]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import (\n",
    "    Map,\n",
    "    Marker,\n",
    "    TileLayer, ImageOverlay,\n",
    "    Polyline, Polygon, Rectangle, Circle, CircleMarker,\n",
    "    GeoJSON,\n",
    "    DrawControl\n",
    ")\n",
    "min_lat,max_lat,min_long,max_long = box = (36.1872, 37.4767, -122.4433, -120.8667)\n",
    "center = [(min_lat+max_lat)/2, (min_long+max_long)/2]\n",
    "zoom = 8\n",
    "\n",
    "m = Map(default_tiles=TileLayer(opacity=1.0), center=center, zoom=zoom)\n",
    "\n",
    "r = Rectangle(bounds=[[min_lat,min_long],[max_lat,max_long]], weight=5, fill_opacity=0.0)\n",
    "# m += r\n",
    "\n",
    "lat_margin=(max_lat-min_lat)/4\n",
    "long_margin=(max_long-min_long)/4\n",
    "circles = []\n",
    "for index,row in all_points.iterrows():\n",
    "    _lat=row['latitude']\n",
    "    _long=row['longitude']\n",
    "#     _count=row['count(station)']\n",
    "#     _coef=row[_avg]\n",
    "    # taking sqrt of count so that the  area of the circle corresponds to the count\n",
    "    c = Circle(location=(_lat,_long), \n",
    "                     radius=15, weight=5,\n",
    "                    color='#F00', opacity=0.8, fill_opacity=0.4,\n",
    "#                     fill_color=get_color(_coef)\n",
    "                    )\n",
    "    circles.append(c)\n",
    "    m.add_layer(c)\n",
    "for point in correlated_points:\n",
    "    latlng = [\n",
    "        (point  [0].latitude.values[0],point[0].longitude.values[0]),\n",
    "        (point  [1].latitude.values[0],point[1].longitude.values[0])\n",
    "    ]\n",
    "#     print latlng\n",
    "    l = Polyline(locations=latlng,weight=1)\n",
    "     \n",
    "#     m+=l\n",
    "    m.add_layer(l)\n",
    "m    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in all_points.iterrows():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation,cor=[],[]\n",
    "for i in range(len(P_norm0)):\n",
    "    for j in range(len(P_norm0)):\n",
    "        elevation += [abs(all_stations[all_stations.index==Stations[i]].elevation.values[0] - all_stations[all_stations.index==Stations[j]].elevation.values[0])]\n",
    "        cor += [P_norm0[i][j]]\n",
    "plt.scatter(cor,elevation)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(arange(1500)),elevations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "print day_station_table.shape\n",
    "P_norm=pd.DataFrame(year_station_table).corr()\n",
    "# print P_norm\n",
    "# print P_norm\n",
    "print type(P_norm),P_norm.shape\n",
    "P_norm = P_norm.as_matrix()\n",
    "for i in range(len(P_norm)):\n",
    "    P_norm[i,i]=np.nan\n",
    "print P_norm.shape\n",
    "A=P_norm.flatten();\n",
    "B=A[~isnan(A)]\n",
    "print A.shape,B.shape\n",
    "hist(B,bins=100);\n",
    "xlabel('significance')\n",
    "# for i in range(len(P_norm)):\n",
    "#     for j in range(len(P_norm)):\n",
    "#         P_norm[i,j] = abs(P_norm[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "P_norm0 = np.nan_to_num(P_norm)\n",
    "n_comp=10\n",
    "pca = PCA(n_components=n_comp, svd_solver='full')\n",
    "pca.fit(P_norm0)     \n",
    "#print(pca.explained_variance_)\n",
    "Var_explained=pca.explained_variance_ratio_\n",
    "plot(insert(cumsum(Var_explained),0,0))\n",
    "grid()\n",
    "# we will look only at the top 4 eigenvectors.\n",
    "n_comp=4\n",
    "pca = PCA(n_components=n_comp, svd_solver='full')\n",
    "pca.fit(P_norm0)     \n",
    "fig,axes=plt.subplots(1,4,figsize=(20,5),sharey='row');\n",
    "L=list(pca.components_.transpose())\n",
    "for i in range(4):\n",
    "    X=sorted(L,key=lambda x:x[i]) \n",
    "    axes[i].plot(X);\n",
    "i=0\n",
    "showmat(P_norm0)\n",
    "for r in range(2):\n",
    "    for c in range(2):\n",
    "        order=np.argsort(pca.components_[i,:])\n",
    "        P_norm_reord=re_order_matrix(P_norm0,order)\n",
    "#         axes[r,c].matshow(P_norm_reord)\n",
    "        showmat(P_norm_reord)\n",
    "        i+=1\n",
    "        break\n",
    "    break\n",
    "correlated=[]\n",
    "\n",
    "for i in range(len(P_norm)):\n",
    "    for j in range(len(P_norm)):\n",
    "        if P_norm[i][j]>0.8:\n",
    "            correlated+=[(i,j)]\n",
    "all_points = all_stations\n",
    "correlated_points = []\n",
    "for cor in correlated:\n",
    "    print cor,\n",
    "    correlated_points +=[ (all_stations[all_stations.index==Stations[cor[0]]],all_stations[all_stations.index==Stations[cor[1]]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import (\n",
    "    Map,\n",
    "    Marker,\n",
    "    TileLayer, ImageOverlay,\n",
    "    Polyline, Polygon, Rectangle, Circle, CircleMarker,\n",
    "    GeoJSON,\n",
    "    DrawControl\n",
    ")\n",
    "min_lat,max_lat,min_long,max_long = box = (36.1872, 37.4767, -122.4433, -120.8667)\n",
    "center = [(min_lat+max_lat)/2, (min_long+max_long)/2]\n",
    "zoom = 8\n",
    "\n",
    "_m = Map(default_tiles=TileLayer(opacity=1.0), center=center, zoom=zoom)\n",
    "\n",
    "r = Rectangle(bounds=[[min_lat,min_long],[max_lat,max_long]], weight=5, fill_opacity=0.0)\n",
    "# m += r\n",
    "\n",
    "lat_margin=(max_lat-min_lat)/4\n",
    "long_margin=(max_long-min_long)/4\n",
    "circles = []\n",
    "for index,row in all_points.iterrows():\n",
    "    _lat=row['latitude']\n",
    "    _long=row['longitude']\n",
    "#     _count=row['count(station)']\n",
    "#     _coef=row[_avg]\n",
    "    # taking sqrt of count so that the  area of the circle corresponds to the count\n",
    "    c = Circle(location=(_lat,_long), \n",
    "                     radius=15, weight=5,\n",
    "                    color='#F00', opacity=0.8, fill_opacity=0.4,\n",
    "#                     fill_color=get_color(_coef)\n",
    "                    )\n",
    "    circles.append(c)\n",
    "    _m.add_layer(c)\n",
    "for point in correlated_points:\n",
    "    latlng = [\n",
    "        (point  [0].latitude.values[0],point[0].longitude.values[0]),\n",
    "        (point  [1].latitude.values[0],point[1].longitude.values[0])\n",
    "    ]\n",
    "#     print latlng\n",
    "    l = Polyline(locations=latlng,weight=1)\n",
    "     \n",
    "#     m+=l\n",
    "    _m.add_layer(l)\n",
    "_m    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correlated_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_norm=pd.DataFrame(year_station_table).corr()\n",
    "# print P_norm\n",
    "# print P_norm\n",
    "print type(P_norm),P_norm.shape\n",
    "P_norm = P_norm.as_matrix()\n",
    "for i in range(len(P_norm)):\n",
    "    P_norm[i,i]=np.nan\n",
    "    \n",
    "P_norm0 = np.nan_to_num(P_norm)\n",
    "elevation,cor=[],[]\n",
    "for i in range(len(P_norm0)):\n",
    "    for j in range(len(P_norm0)):\n",
    "        elevation += [abs(all_stations[all_stations.index==Stations[i]].elevation.values[0] - all_stations[all_stations.index==Stations[j]].elevation.values[0])]\n",
    "        cor += [P_norm0[i][j]]\n",
    "plt.scatter(cor,elevation)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "118px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "state": {
    "0d4726d074414304b7910c9bc9aee2a0": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "9dfb4bbaf8664891a93b62da7476d8fe": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
