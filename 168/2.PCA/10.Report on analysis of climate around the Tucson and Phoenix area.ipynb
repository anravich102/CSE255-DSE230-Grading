{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "![PRCP(mean+Std).png](Plots/PRCP(mean+Std).png)\n",
      "![PRCP_Var_Explained.png](Plots/PRCP_Var_Explained.png)\n",
      "![PRCP_years.png](Plots/PRCP_years.png)\n",
      "![SNOW(mean+Std).png](Plots/SNOW(mean+Std).png)\n",
      "![Snow_Var_explained.png](Plots/Snow_Var_explained.png)\n",
      "![SNWD(mean+Std).png](Plots/SNWD(mean+Std).png)\n",
      "![SNWD_Var_Exlained.png](Plots/SNWD_Var_Exlained.png)\n",
      "![TMAX(mean+Std).png](Plots/TMAX(mean+Std).png)\n",
      "![TMax_Var_explained.png](Plots/TMax_Var_explained.png)\n",
      "![TMIN.png](Plots/TMIN.png)\n",
      "![TMin_Var_explained.png](Plots/TMin_Var_explained.png)\n",
      "![TOBS(mean+std).png](Plots/TOBS(mean+std).png)\n",
      "![TOBS_Var_Explained.png](Plots/TOBS_Var_Explained.png)\n",
      "![Top_Eigen_Vecs_PRCP.png](Plots/Top_Eigen_Vecs_PRCP.png)\n",
      "![Top_Eigen_Vecs_SNOW.png](Plots/Top_Eigen_Vecs_SNOW.png)\n",
      "![Top_Eigen_Vecs_SNWD.png](Plots/Top_Eigen_Vecs_SNWD.png)\n",
      "![Top_Eigen_Vecs_TMAX.png](Plots/Top_Eigen_Vecs_TMAX.png)\n",
      "![Top_Eigen_Vecs_TMIN.png](Plots/Top_Eigen_Vecs_TMIN.png)\n",
      "![Top_Eigen_Vecs_TOBS.png](Plots/Top_Eigen_Vecs_TOBS.png)\n"
     ]
    }
   ],
   "source": [
    "L=!ls Plots/\n",
    "for l in L:\n",
    "    print \"![%s](Plots/%s)\"%(l,l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Arizona(nearBy Tucson and Phoenix) Weather Analysis\n",
    "\n",
    "This is a report on the historical analysis of weather patterns in an area that lies in the state of Arizona around Tucson and Phoenix.\n",
    "\n",
    "The data we will use here comes from [NOAA](https://www.ncdc.noaa.gov/). Specifically, it was downloaded from This [FTP site](ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/).\n",
    "\n",
    "We focused on six measurements:\n",
    "* **TMIN, TMAX:** the daily minimum and maximum temperature(degrees C).\n",
    "* **TOBS:** The average temperature for each day(degrees C).\n",
    "* **PRCP:** Daily Percipitation (in tenths of mm)\n",
    "* **SNOW:** Daily snowfall (in mm)\n",
    "* **SNWD:** The depth of accumulated snow(in mm).\n",
    "\n",
    "## Sanity-check: comparison with outside sources\n",
    "\n",
    "<p>We start by comparing some of the general statistics with graphs that we obtained from a site called <a href=\"http://www.usclimatedata.com/climate/tucson/arizona/united-states/usaz0247\" target=\"_blank\">US Climate Data</a> The graph below shows the daily minimum and maximum temperatures for each month, as well as the total precipitation for each month.</p>\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<p><img alt=\"Climate_Tucson_-_Arizona_and_Weather_averages_Tucson.jpg\" src=\"Plots/TucsonUSdata.png\" /></p>\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<p>We see that the min and max daily&nbsp;temperature agree with the ones we got from our data.</p>\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<p><img alt=\"TMIN,TMAX.png\" src=\"Plots/TMIN.png\" style=\"height:300px; width:800px\" /></p>\n",
    "\n",
    "<p>To compare the precipitation&nbsp;we need to translate tenths of millimeter/day to millimeters/month. According to our analysis the average rainfall is 1.00 mm/day(given units are tenths of millimeter hence a unit of 10 will correspond to 1 mm) and (considering average through out the year) which translates to about 30 mm&nbsp;per month. According to US-Climate-Data the average rainfall is closer to around 30mm per month. More importantly the peaks in the precipatation are happening during the same period in the year as per both graphs.</p>\n",
    "\n",
    "<p>&nbsp;<img alt=\"PRCP.png\" src=\"Plots/PRCP+mean.png\" style=\"height:450px; width:600px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## PCA analysis\n",
    "\n",
    "For each of the six measurement, we compute the percentate of the variance explained as a function of the number of eigen-vectors used.\n",
    "\n",
    "### Percentage of variance explained.\n",
    "![TMin_Var_explained.png](Plots/TMin_Var_explained.png)\n",
    "We see that the top 5 eigen-vectors explain 48% of variance for TMIN, 60% for TOBS and 46% for TMAX.\n",
    "\n",
    "We conclude that of the three, TOBS is best explained by the top 5 eigenvectors. However TMIN and TMAX explanations are satisfactory as well. The remarkable thing is the proportion of variance explained by the first eigen vector(for example it explains 50% variance for TOBS) in all these cases. As Arizona is dry and hot most of the year, the temperatures follow the similar trend, hence less noisy or less unpredictable. Hence variance explained by first few eigen vectors is large. \n",
    "\n",
    "\n",
    "![Snow_Var_explained.png](Plots/Snow_Var_explained.png)\n",
    "\n",
    "The top 5 eigenvectors explain 8% of the variance for PRCP and 16% for SNOW. Both are low values. On the other hand the top 5 eigenvectors explain 85% of the variance for SNWD. This means that these top 5 eigenvectors capture most of the variation in the snow signals.\n",
    "\n",
    "It makes sense that SNWD would be less noisy than SNOW. That is because SNWD is a decaying integral of SNOW and, as such, varies less between days and between the same date on diffferent years.\n",
    "\n",
    "![SNWD_mean.png](Plots/SNWD_mean.png)\n",
    "\n",
    "However, the peak in the mean of snowdepth is around 2.5mm as shown in the figure above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Snow Depth(SNWD)\n",
    "\n",
    "We choose to analyze the eigen-decomposition for SNWD because the first 5 eigen-vectors explain more than 80% of the variance and it is the one of the best explained by first few eigen vectors among the other measurements.\n",
    "\n",
    "First, we graph the mean and the top 3 eigen-vectors.\n",
    "\n",
    "This region has high snow(in relative sense) from January to March and also from November to December.\n",
    "![SNWD_mean_1.png](Plots/SNWD_mean_1.png)\n",
    "\n",
    "Since this area rarely snows, the eigen vectors are sparse, even the target vectors are sparse, hence it is not meaningful to continue PCA analysis on snow depths, rather we turn our attention to a more meaningful measurement in this dry and hot region like Tucson i.e. TOBS. A sample recostruction of the target snow depth vector is shown below. Notice the sparsity of the target vectors and the eigen vectors\n",
    "\n",
    "![SNWD_Eigen_Constru.png](Plots/SNWD_Eigen_Constru.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Analysis of Observed Temperature(TOBS)\n",
    "\n",
    "We choose to analyze the eigen-decomposition for TOBS because the first 5 eigen-vectors explain 60% of the variance and it is the one best explained by first few eigen vectors among the other measurements.\n",
    "\n",
    "First, we graph the mean and the top 5 eigen-vectors. To avoid the graphs being too clumsy we plot first set of three eigen vectors(1,2,3) in one plot and next set(3,4,5) in another plot with eigen vector 3 present in both plots.\n",
    "\n",
    "This region has high temperatures from May to September where the peak is at the end of June.\n",
    "![TOBS.png](Plots/TOBS.png)\n",
    "![3_4_5_eigs_TOBS.png](Plots/3_4_5_eigs_TOBS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Now we will move on to interpret the eigen-vectors. The first eigen-function (eig1) has a shape similar to a line parallel to x-axis, and also this eigen vector alone explains more than 50% of the variance. So the effect of this eigen vector on the reconstruction of the target vector would be just shifting the mean by some constant times the eigen vector. So in this case where variance explained by first eigen vector is large we can compare two data points by their first eigen coefficient and determine if they are hotter(Coefficient is positive) or cooler(Coefficient is negative) with respect to the mean and also comparisions among the data points can also be made similarly. \n",
    "\n",
    "Another interpretation of this shape is that eig1 represents the temperature above/below the mean(by sign of the coefficient), but without changing the distribution over time. Since Arizona is a dry place through the year, the scale of variation is almost the same across all days as depticted by eigen vector being parallel to x-axis(days).\n",
    "Also, since July is the summer the variation is even less across these days indicated by dip of the eigen vector arounf July.\n",
    "\n",
    "**eig2,eig3, eig4 and eig5** are similar in the following way. They all oscilate between positive and negative values.\n",
    "However eig2 explains a bit more in the sense, its shape is bottom-up version of mean. This means that even though the direction is largest variance has almost same effect on the all days, direction of second largest variance has more effect on the start and end of the year. This essentially captures the variations around the days where it is generally ought to be cooler i.e.. at the start and end of the year where it is generally cooler and if there are variations in those temperatures, that variance is captured by second largest eigen vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Examples of reconstructions\n",
    "\n",
    "#### Coeff1\n",
    "Coeff1: most negative\n",
    "![Eigen_Reconstruct_TOBS_neg_Coeff1.png](Plots/Eigen_Reconstruct_TOBS_neg_Coeff1.png)\n",
    "Coeff1: most positive\n",
    "![Eigen_Reconstruct_TOBS_pos_Coeff1.png](Plots/Eigen_Reconstruct_TOBS_pos_Coeff1.png)\n",
    "As per our comments made earlier Large positive values of coeff1 correspond to data points that are hotter compared to mean and Low values correspond to data points that are cooler compared to mean. Also the approximation through reconstruction is good justifying our direction of approach(that we took based on top few eigen vectors explaining good amount of variance).\n",
    "\n",
    "#### Coeff2\n",
    "Coeff2: most negative\n",
    "![Eigen_Reconstruct_TOBS_neg_Coeff2.png](Plots/Eigen_Reconstruct_TOBS_neg_Coeff2.png)\n",
    "Coeff2: most positive\n",
    "![Eigen_Reconstruct_TOBS_pos_Coeff2.png](Plots/Eigen_Reconstruct_TOBS_pos_Coeff2.png)\n",
    "\n",
    "Large positive values of coeff2 mostly to correspond temperatures above mean and Negative values for coeff2 correspond to temperatures below mean at the start and end of the year. \n",
    "\n",
    "#### CDF of Res-3 for TOBS\n",
    "![res3_TOBS.png](Plots/res3_TOBS.png)\n",
    "\n",
    "As a conclusion looking at the residual obtained after approximation through three eigen vectors shows that the number of target data points that the first few eigen vectors could explain leaving small residuals is close to 50%.\n",
    "\n",
    "#### Average(Coefficient_1) vs Elevation for TOBS\n",
    "As we pointed out earlier that coefficient_1 represents the magnitude of temperature above the mean value, we are interested in finding how the place being hotter or cooler than the mean temperature is related to the elevation. The  following figure describes such a relationship.\n",
    "\n",
    "![elve.png](Plots/elve.png)\n",
    "\n",
    "\n",
    "We can see that(roughly) as the elevation increases a place becomes cooler compared to the mean temperature, which is highly intuitive.\n",
    "\n",
    "\n",
    "#### Map showing distribution of TOBS\n",
    "\n",
    "![MAP_TOBS.png](Plots/MAP_TOBS.png)\n",
    "\n",
    "We can see there are abundant measurements that are available to us, hence our data analysis is statistically significant and also temperature of most of the places is more.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Bad approximations from eigen vectors for Precipitation(PRCP)\n",
    "\n",
    "In this case we will look at what happens if we try to approximate the precipitation using reconstruction through eigen vectors. We already know that the variance explained by first 5 eigen vectors for precipitation is just 9%. So our intial intuition should suggest that approximations shouls be bad. Let's see.\n",
    "\n",
    "First, we graph the mean and the top 3 eigen-vectors.\n",
    "![PRCP.png](Plots/PRCP.png) \n",
    "So the general trend is, it rains more in july and moderate rains happen at the start and end of the year. This region has an average of around 1mm per day(given units are tenths of mm) which is equivalent to 13 inches per year where as US national average is 39 inches per year. So the rainfall is relatively less in this region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Examples of Bad Reconstructions\n",
    "\n",
    "Lowest Residual-3 Values\n",
    "![PRCP_reconstruct.png](Plots/PRCP_reconstruct.png)\n",
    "\n",
    "Highest Residual-3 Values\n",
    "![PRCP_reconstruct_bad.png](Plots/PRCP_reconstruct_bad.png)\n",
    "\n",
    "As we can see even in the case of lowest residual-3, the approximations aren't good, hence needless to say in the next set of plots where these have highest residual-3 values the approximations are even bad. This is in agreement with our intuition that if percent variance explained by first few eigen vectors is not large, their approximations will not be good as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The variation in the Temperature is mostly due to station than year. \n",
    "\n",
    "In the previous section we see the variation of Coeff1, which corresponds to the temperature above the mean, with respect to location. We now estimate the relative importance of location-to-location variation relative to year-by-year variation.\n",
    "\n",
    "These are measured using the fraction by which the variance is reduced when we subtract from each station/year entry the average-per-year or the average-per-station respectively. Here are the results:\n",
    "\n",
    "** coeff_1 **  \n",
    "total RMS                   =  848.744884889\n",
    "RMS removing mean-by-station=  536.047808061, fraction explained = 36.8\n",
    "RMS removing mean-by-year   =  816.672499819, fraction explained = 3.7 \n",
    "\n",
    "** coeff_2 **  \n",
    "total RMS                   =  263.799039734\n",
    "RMS removing mean-by-station=  210.769809471, fraction explained = 2.01\n",
    "RMS removing mean-by-year   =  225.212867467, fraction explained = 1.44 \n",
    "\n",
    "** coeff_3 **  \n",
    "total RMS                   =  195.130858239\n",
    "RMS removing mean-by-station=  186.47253319, fraction explained = 4.6\n",
    "RMS removing mean-by-year   =  143.104918146, fraction explained = 26.66 \n",
    "\n",
    "\n",
    "For coefficient1 which represents the temperature above the mean, variations by station explains more than variations by year, from this we can infer that the effect of stations on temperature is more than that of the year. coefficient2 is explained in the same way by year and stations. This corresponds to eigen vector which explains the variations at the start and end of the year. Although for coefficient 3, variations by year explains more than variations by station, we couldn't interpret its meaning as its corresponding eig-3 can't be interpreted as well. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Analysis of correlation between percipitation across locations\n",
    "\n",
    "We have already looked into the mean and eigen vectors for the precipitation in the section depicting bad approximations through reconstructions. The top few eigen vectors failed to explain significant amount of variance. The same can be seen through a cumulative distribution of residual-3.As we can see more than 80% of the data points have res3 close to 0.95 which is not great for one intending to do dimensionality reduction.\n",
    "\n",
    "![res3_PRCP.png](Plots/res3_PRCP.png)\n",
    "\n",
    "Also, if we look at the cdf of rainfall we will see that on majority of days, it won't rain and when it rains occasionally it may rain heavily.\n",
    "\n",
    "![cdf_rainfall.png](Plots/cdf_rainfall.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map showing precipitations\n",
    "\n",
    "![Map_PRCP.png](Plots/Map_PRCP.png) \n",
    "\n",
    "As we can see from the map, there are many stations with large number of measurements but since rainfall in this region is not common, there are very blue circles in the map. Interesting observation is that these rainfall regions are close to Gila National Forest and Tonto National Forest and we know the fact that rainfall near forests is high(represented by blue color)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "It is likely to be hard to find correlations between the amount of rain on the same day in different stations. Because amounts of rain vary a lot between even close locations. It is more reasonable to try to compare whether or not it rained on the same day in different stations. As we see from the graph above, in our region it rains in about one tenth of the days.\n",
    "\n",
    "\n",
    "### Measuring statistical significance\n",
    "\n",
    "We want to find a statistical test for rejecting the null hypothesis that says that the rainfall in the two locations is independent.\n",
    "\n",
    "Using the inner product is too noisy, because you multiply the rainfall on the same day in two locations and that product can be very large - leading to a large variance and poor ability to discriminate.\n",
    "\n",
    "An alternative is to ignore the amount of rain, and just ask whether it rained in both locations. We can then compute the probability associated with the number of overlaps under the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Calculation Methodology\n",
    "\n",
    "Fix two stations. We restrict our attention to the days for which we have measurements for both stations, and define the following notation:\n",
    "* $m$ : the total number of days (for which we have measurements for both stations).\n",
    "* $n_1$ : the number of days that it rained on station 1\n",
    "* $n_2$ : the number of days that it rained on station 2\n",
    "* $l$ : the number of days that it rained on both stations.\n",
    "\n",
    "We want to calculate the probability that the number of overlap days is $l$ given $m,n_1,n_2$.\n",
    "\n",
    "The answer is:\n",
    "$$\n",
    "P = {m \\choose l,n_1-l,n_2-l,m-n_1-n_2+l} /{m \\choose n_1}{m \\choose n_2}\n",
    "$$\n",
    "\n",
    "Where\n",
    "$$\n",
    "{m \\choose l,n_1-l,n_2-l,m-n_1-n_2+l} = \\frac{m!}{l! (n_1-l)! (n_2-l)! (m-n_1-n_2+l)!}\n",
    "$$\n",
    "\n",
    "We use the fact that $\\Gamma(n+1) = n!$ and denote $G(n) \\doteq \\log \\Gamma(n+1)$\n",
    "$$\n",
    "\\log P = \\left[G(m) - G(l) -G(n_1-l) -G(n_2-l) -G(m-n_1-n_2+l) \\right] - \n",
    "\\left[G(m)-G(n_1)-G(m-n_1)\\right] - \\left[G(m)-G(n_2)-G(m-n_2)\\right]\n",
    "$$\n",
    "Which slightly simplifies to \n",
    "$$\n",
    "\\log P = -G(l) -G(n_1-l) -G(n_2-l) -G(m-n_1-n_2+l) - G(m)+G(n_1)+G(m-n_1) +G(n_2)+G(m-n_2)\n",
    "$$\n",
    "\n",
    "The log probability scales with $m$ the length of the overlap. So to get a per-day significance we consider $\n",
    "\\frac{1}{m} \\log P $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Correlation Matrix\n",
    "\n",
    "As we see from distribution of correlations of pairs of stations, most of the log probabilities have low negative values, suggesting possibility of high correlations. \n",
    "\n",
    "\n",
    "![significance_hist1.png](Plots/significance_hist1.png) \n",
    "\n",
    "Also, there are two peaks and others crowded around them as shown in the histogram plot. So this suggests there are two large groups of stations that are highly correlated among themselves.\n",
    "\n",
    "The above statement is justified by the correlation matrix shown below where the first ~80 stations are highly correlated and next ~70 stations have good correlation as well.\n",
    "\n",
    "The other white spots in the matrix are the nan's and diagonal elements represent the correlation of the station with itself.\n",
    "\n",
    "![correl_mat.png](Plots/correl_mat.png)\n",
    "\n",
    "\n",
    "### Finding structure in the rependency matrix.\n",
    "The matrix above shows, for each pair of stations, the normalized log probability that the overlap in rain days is high.\n",
    "\n",
    "We see immediately the first 80 stations are most highly correlated with each other. \n",
    "\n",
    "To find more correlations we use SVD (the term PCA is reserved for decomposition of the covariance matrix). As we see from below that the top 10 eigenvectors explain about 80% of the square magnitude of the matrix.\n",
    "\n",
    "![repedmat.png](Plots/repedmat.png)\n",
    "\n",
    "The below plot shows the plot of first 4 eigen vectors sorted accoring to its significance in terms of its eigen value.\n",
    "\n",
    "![eig_vecs_repd.png](Plots/eig_vecs_repd.png)\n",
    "\n",
    "\n",
    "The below plot shows the orderings of the probability matrix according to the eigen vector.\n",
    "\n",
    "![pnormmat.png](Plots/pnormmat.png)\n",
    "\n",
    "When we reorder the rows and columns of the matrix using one of the eigenvectors, the grouping of the stations becomes more evident. For example, consider the upper left corner of the upper left matrix. The stations at positions 0-60 are clearly strongly correlated with each other.This type of organization is called Block Diagonal and it typically reveals important structure such as grouping or clustering.It may be more helpful to extract the sets of stations that form blocks like this, and then plot them on the map to see their spatial relationship.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
